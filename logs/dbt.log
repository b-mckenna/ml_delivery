

============================== 2022-11-01 19:47:27.367708 | 54990147-e605-4369-bcf9-1793e2488b11 ==============================
[0m19:47:27.367718 [info ] [MainThread]: Running with dbt=1.3.0
[0m19:47:27.368051 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m19:47:27.368188 [debug] [MainThread]: Tracking: tracking
[0m19:47:27.380726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082ae970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082aec10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082aef70>]}
[0m19:47:27.881246 [debug] [MainThread]: Executing "git --help"
[0m19:47:27.890751 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m19:47:27.891742 [debug] [MainThread]: STDERR: "b''"
[0m19:47:27.900749 [debug] [MainThread]: Acquiring new databricks connection "debug"
[0m19:47:27.902898 [debug] [MainThread]: Using databricks connection "debug"
[0m19:47:27.903166 [debug] [MainThread]: On debug: select 1 as id
[0m19:47:27.903465 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:47:32.178105 [debug] [MainThread]: SQL status: OK in 4.27 seconds
[0m19:47:32.180475 [debug] [MainThread]: On debug: Close
[0m19:47:32.807721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c99d0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c98c4f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11c98c820>]}
[0m19:47:32.808498 [debug] [MainThread]: Flushing usage events
[0m19:47:32.982219 [debug] [MainThread]: Connection 'debug' was properly closed.


============================== 2022-11-02 13:07:22.904364 | 744836a7-c728-4180-90c1-b69a20a5a68e ==============================
[0m13:07:22.904419 [info ] [MainThread]: Running with dbt=1.3.0
[0m13:07:22.905711 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m13:07:22.905903 [debug] [MainThread]: Tracking: tracking
[0m13:07:22.926992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121321310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121321490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121321430>]}
[0m13:07:22.949334 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m13:07:22.949652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '744836a7-c728-4180-90c1-b69a20a5a68e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12133a130>]}
[0m13:07:23.005667 [debug] [MainThread]: Parsing macros/statement.sql
[0m13:07:23.011349 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m13:07:23.020912 [debug] [MainThread]: Parsing macros/catalog.sql
[0m13:07:23.023673 [debug] [MainThread]: Parsing macros/adapters.sql
[0m13:07:23.048242 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m13:07:23.056193 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m13:07:23.056718 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m13:07:23.060329 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m13:07:23.078317 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m13:07:23.079798 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m13:07:23.087002 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m13:07:23.087929 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m13:07:23.088980 [debug] [MainThread]: Parsing macros/adapters.sql
[0m13:07:23.128579 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m13:07:23.131398 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m13:07:23.139750 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m13:07:23.140238 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m13:07:23.145780 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m13:07:23.169165 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m13:07:23.171045 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m13:07:23.175563 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m13:07:23.183240 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m13:07:23.189795 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m13:07:23.190263 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m13:07:23.191377 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m13:07:23.196245 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m13:07:23.196576 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m13:07:23.198128 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m13:07:23.209628 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m13:07:23.210046 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m13:07:23.210539 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m13:07:23.210952 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m13:07:23.212078 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m13:07:23.212550 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m13:07:23.213369 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m13:07:23.216726 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m13:07:23.218691 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m13:07:23.220122 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m13:07:23.233946 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m13:07:23.245098 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m13:07:23.255514 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m13:07:23.259269 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m13:07:23.260898 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m13:07:23.262423 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m13:07:23.269253 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m13:07:23.282911 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m13:07:23.284171 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m13:07:23.290143 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m13:07:23.299028 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m13:07:23.313481 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m13:07:23.318109 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m13:07:23.321081 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m13:07:23.325751 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m13:07:23.326838 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m13:07:23.329640 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m13:07:23.331588 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m13:07:23.337460 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m13:07:23.353122 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m13:07:23.354401 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m13:07:23.356475 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m13:07:23.357877 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m13:07:23.358608 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m13:07:23.359289 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m13:07:23.359873 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m13:07:23.361020 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m13:07:23.366328 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m13:07:23.373529 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m13:07:23.374245 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m13:07:23.375403 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m13:07:23.376251 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m13:07:23.377071 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m13:07:23.378137 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m13:07:23.378860 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m13:07:23.379867 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m13:07:23.380917 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m13:07:23.382888 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m13:07:23.383945 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m13:07:23.384883 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m13:07:23.385809 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m13:07:23.386720 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m13:07:23.387613 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m13:07:23.388551 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m13:07:23.389362 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m13:07:23.395290 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m13:07:23.396262 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m13:07:23.397065 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m13:07:23.398612 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m13:07:23.400482 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m13:07:23.401388 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m13:07:23.402667 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m13:07:23.403727 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m13:07:23.405564 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m13:07:23.408715 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m13:07:23.411189 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m13:07:23.424047 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m13:07:23.425701 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m13:07:23.437481 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m13:07:23.441188 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m13:07:23.447596 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m13:07:23.456020 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m13:07:23.461233 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m13:07:23.788230 [debug] [MainThread]: 1699: static parser successfully parsed ddl.sql
[0m13:07:23.801933 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m13:07:23.805135 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m13:07:23.878602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '744836a7-c728-4180-90c1-b69a20a5a68e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0baa60>]}
[0m13:07:23.888127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '744836a7-c728-4180-90c1-b69a20a5a68e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12133ad00>]}
[0m13:07:23.888441 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:07:23.888682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '744836a7-c728-4180-90c1-b69a20a5a68e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121516fd0>]}
[0m13:07:23.890065 [info ] [MainThread]: 
[0m13:07:23.890675 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m13:07:23.891735 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m13:07:23.902370 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:23.902533 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:07:23.902679 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m13:07:23.902842 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:07:27.722303 [debug] [ThreadPool]: SQL status: OK in 3.82 seconds
[0m13:07:27.764351 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:07:27.764602 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m13:07:37.822144 [debug] [ThreadPool]: SQL status: OK in 10.06 seconds
[0m13:07:38.299895 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m13:07:38.300271 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:07:38.300500 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m13:07:38.834026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '744836a7-c728-4180-90c1-b69a20a5a68e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12149b040>]}
[0m13:07:38.834556 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:38.834747 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:07:38.835380 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:07:38.835709 [info ] [MainThread]: 
[0m13:07:38.842392 [debug] [Thread-1  ]: Began running node test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:07:38.842701 [info ] [Thread-1  ]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m13:07:38.843504 [debug] [Thread-1  ]: Acquiring new databricks connection "test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710"
[0m13:07:38.843683 [debug] [Thread-1  ]: Began compiling node test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:07:38.843862 [debug] [Thread-1  ]: Compiling test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:07:38.861504 [debug] [Thread-1  ]: Writing injected SQL for node "test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710"
[0m13:07:38.862275 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:38.862461 [debug] [Thread-1  ]: Began executing node test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:07:38.878965 [debug] [Thread-1  ]: Writing runtime sql for node "test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710"
[0m13:07:38.879812 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:38.879970 [debug] [Thread-1  ]: Using databricks connection "test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710"
[0m13:07:38.880130 [debug] [Thread-1  ]: On test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from hive_metastore.default.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m13:07:38.880267 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:07:39.822824 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from hive_metastore.default.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m13:07:39.823205 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: hive_metastore.default.my_first_dbt_model; line 14 pos 5
[0m13:07:39.823485 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_first_dbt_model; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_first_dbt_model; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m13:07:39.823701 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'!D\xe2\x16\xccXG\xe0\xad{@\xb2\xd3\xf6\xed}'
[0m13:07:39.824032 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:39.824234 [debug] [Thread-1  ]: On test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710: ROLLBACK
[0m13:07:39.824399 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:07:39.824557 [debug] [Thread-1  ]: On test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m13:07:40.286907 [debug] [Thread-1  ]: Runtime Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  Table or view not found: hive_metastore.default.my_first_dbt_model; line 14 pos 5
[0m13:07:40.287430 [error] [Thread-1  ]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.44s]
[0m13:07:40.288027 [debug] [Thread-1  ]: Finished running node test.ml_delivery.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:07:40.288332 [debug] [Thread-1  ]: Began running node test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778
[0m13:07:40.288730 [info ] [Thread-1  ]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m13:07:40.289501 [debug] [Thread-1  ]: Acquiring new databricks connection "test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778"
[0m13:07:40.289697 [debug] [Thread-1  ]: Began compiling node test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778
[0m13:07:40.289932 [debug] [Thread-1  ]: Compiling test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778
[0m13:07:40.296829 [debug] [Thread-1  ]: Writing injected SQL for node "test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778"
[0m13:07:40.297292 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:40.297476 [debug] [Thread-1  ]: Began executing node test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778
[0m13:07:40.300216 [debug] [Thread-1  ]: Writing runtime sql for node "test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778"
[0m13:07:40.300670 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:40.300827 [debug] [Thread-1  ]: Using databricks connection "test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778"
[0m13:07:40.300991 [debug] [Thread-1  ]: On test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from hive_metastore.default.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m13:07:40.301131 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:07:41.323662 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from hive_metastore.default.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m13:07:41.324039 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: hive_metastore.default.my_second_dbt_model; line 14 pos 5
[0m13:07:41.324257 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_second_dbt_model; line 14 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_second_dbt_model; line 14 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m13:07:41.324523 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xc79$\xc0\xc0cMq\xbb\x14\xe7\xfb\x19\xff\xd1\x9f'
[0m13:07:41.324855 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:41.325105 [debug] [Thread-1  ]: On test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778: ROLLBACK
[0m13:07:41.325310 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:07:41.325503 [debug] [Thread-1  ]: On test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778: Close
[0m13:07:41.700276 [debug] [Thread-1  ]: Runtime Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  Table or view not found: hive_metastore.default.my_second_dbt_model; line 14 pos 5
[0m13:07:41.700748 [error] [Thread-1  ]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.41s]
[0m13:07:41.701299 [debug] [Thread-1  ]: Finished running node test.ml_delivery.not_null_my_second_dbt_model_id.151b76d778
[0m13:07:41.701599 [debug] [Thread-1  ]: Began running node test.ml_delivery.unique_my_first_dbt_model_id.16e066b321
[0m13:07:41.701963 [info ] [Thread-1  ]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m13:07:41.702830 [debug] [Thread-1  ]: Acquiring new databricks connection "test.ml_delivery.unique_my_first_dbt_model_id.16e066b321"
[0m13:07:41.703031 [debug] [Thread-1  ]: Began compiling node test.ml_delivery.unique_my_first_dbt_model_id.16e066b321
[0m13:07:41.703215 [debug] [Thread-1  ]: Compiling test.ml_delivery.unique_my_first_dbt_model_id.16e066b321
[0m13:07:41.713407 [debug] [Thread-1  ]: Writing injected SQL for node "test.ml_delivery.unique_my_first_dbt_model_id.16e066b321"
[0m13:07:41.713860 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:41.714050 [debug] [Thread-1  ]: Began executing node test.ml_delivery.unique_my_first_dbt_model_id.16e066b321
[0m13:07:41.716755 [debug] [Thread-1  ]: Writing runtime sql for node "test.ml_delivery.unique_my_first_dbt_model_id.16e066b321"
[0m13:07:41.717210 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:41.717371 [debug] [Thread-1  ]: Using databricks connection "test.ml_delivery.unique_my_first_dbt_model_id.16e066b321"
[0m13:07:41.717539 [debug] [Thread-1  ]: On test.ml_delivery.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from hive_metastore.default.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:41.717682 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:07:42.662718 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.unique_my_first_dbt_model_id.16e066b321"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from hive_metastore.default.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:42.663153 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: hive_metastore.default.my_first_dbt_model; line 15 pos 5
[0m13:07:42.663377 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_first_dbt_model; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_first_dbt_model; line 15 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m13:07:42.663585 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'\xc2/\xf9\rY\x87H\x88\xb4~\x86\xc2\xa4wM\x8b'
[0m13:07:42.663907 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:42.664152 [debug] [Thread-1  ]: On test.ml_delivery.unique_my_first_dbt_model_id.16e066b321: ROLLBACK
[0m13:07:42.664358 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:07:42.664552 [debug] [Thread-1  ]: On test.ml_delivery.unique_my_first_dbt_model_id.16e066b321: Close
[0m13:07:43.050738 [debug] [Thread-1  ]: Runtime Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  Table or view not found: hive_metastore.default.my_first_dbt_model; line 15 pos 5
[0m13:07:43.051188 [error] [Thread-1  ]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 1.35s]
[0m13:07:43.051754 [debug] [Thread-1  ]: Finished running node test.ml_delivery.unique_my_first_dbt_model_id.16e066b321
[0m13:07:43.052118 [debug] [Thread-1  ]: Began running node test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493
[0m13:07:43.052489 [info ] [Thread-1  ]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m13:07:43.053341 [debug] [Thread-1  ]: Acquiring new databricks connection "test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493"
[0m13:07:43.053540 [debug] [Thread-1  ]: Began compiling node test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493
[0m13:07:43.053726 [debug] [Thread-1  ]: Compiling test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493
[0m13:07:43.060637 [debug] [Thread-1  ]: Writing injected SQL for node "test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493"
[0m13:07:43.061095 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:43.061275 [debug] [Thread-1  ]: Began executing node test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493
[0m13:07:43.063988 [debug] [Thread-1  ]: Writing runtime sql for node "test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493"
[0m13:07:43.064432 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:43.064592 [debug] [Thread-1  ]: Using databricks connection "test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493"
[0m13:07:43.064759 [debug] [Thread-1  ]: On test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from hive_metastore.default.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:43.064903 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:07:43.915035 [debug] [Thread-1  ]: Databricks adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from hive_metastore.default.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:43.915418 [debug] [Thread-1  ]: Databricks adapter: <class 'databricks.sql.exc.ServerOperationError'>: Table or view not found: hive_metastore.default.my_second_dbt_model; line 15 pos 5
[0m13:07:43.915642 [debug] [Thread-1  ]: Databricks adapter: diagnostic-info: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_second_dbt_model; line 15 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:47)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:435)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:257)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:123)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:48)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:52)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:235)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:220)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:269)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Table or view not found: hive_metastore.default.my_second_dbt_model; line 15 pos 5
	at org.apache.spark.sql.AnalysisException.copy(AnalysisException.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:427)
	... 16 more

[0m13:07:43.915921 [debug] [Thread-1  ]: Databricks adapter: operation-id: b'%\x13\xb8\xf6\xa4\xe4@\xd4\x9cX\xee \x17r\xe0;'
[0m13:07:43.916257 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:43.916509 [debug] [Thread-1  ]: On test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493: ROLLBACK
[0m13:07:43.916725 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:07:43.916926 [debug] [Thread-1  ]: On test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m13:07:44.295390 [debug] [Thread-1  ]: Runtime Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  Table or view not found: hive_metastore.default.my_second_dbt_model; line 15 pos 5
[0m13:07:44.295857 [error] [Thread-1  ]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 1.24s]
[0m13:07:44.296421 [debug] [Thread-1  ]: Finished running node test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493
[0m13:07:44.298010 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m13:07:44.298239 [debug] [MainThread]: On master: ROLLBACK
[0m13:07:44.298411 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:07:44.704102 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:07:44.704495 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:07:44.704714 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:07:44.704943 [debug] [MainThread]: On master: ROLLBACK
[0m13:07:44.705146 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:07:44.705341 [debug] [MainThread]: On master: Close
[0m13:07:45.081906 [info ] [MainThread]: 
[0m13:07:45.082401 [info ] [MainThread]: Finished running 4 tests in 0 hours 0 minutes and 21.19 seconds (21.19s).
[0m13:07:45.082778 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:07:45.082982 [debug] [MainThread]: Connection 'test.ml_delivery.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m13:07:45.096199 [info ] [MainThread]: 
[0m13:07:45.096485 [info ] [MainThread]: [31mCompleted with 4 errors and 0 warnings:[0m
[0m13:07:45.096771 [info ] [MainThread]: 
[0m13:07:45.097006 [error] [MainThread]: [33mRuntime Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m13:07:45.097235 [error] [MainThread]:   Table or view not found: hive_metastore.default.my_first_dbt_model; line 14 pos 5
[0m13:07:45.097451 [info ] [MainThread]: 
[0m13:07:45.097667 [error] [MainThread]: [33mRuntime Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m13:07:45.097924 [error] [MainThread]:   Table or view not found: hive_metastore.default.my_second_dbt_model; line 14 pos 5
[0m13:07:45.098136 [info ] [MainThread]: 
[0m13:07:45.098351 [error] [MainThread]: [33mRuntime Error in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m13:07:45.098564 [error] [MainThread]:   Table or view not found: hive_metastore.default.my_first_dbt_model; line 15 pos 5
[0m13:07:45.098775 [info ] [MainThread]: 
[0m13:07:45.098987 [error] [MainThread]: [33mRuntime Error in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m13:07:45.099199 [error] [MainThread]:   Table or view not found: hive_metastore.default.my_second_dbt_model; line 15 pos 5
[0m13:07:45.099415 [info ] [MainThread]: 
[0m13:07:45.099635 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 TOTAL=4
[0m13:07:45.099947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1215e5d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1216b4340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1216b4a60>]}
[0m13:07:45.100201 [debug] [MainThread]: Flushing usage events


============================== 2022-11-02 13:08:19.610397 | 39cf5fbf-a776-4982-8aac-db1de3138fe1 ==============================
[0m13:08:19.610456 [info ] [MainThread]: Running with dbt=1.3.0
[0m13:08:19.611183 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:08:19.611342 [debug] [MainThread]: Tracking: tracking
[0m13:08:19.623219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d621370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6214f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d621490>]}
[0m13:08:19.692266 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:08:19.692450 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:08:19.698445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '39cf5fbf-a776-4982-8aac-db1de3138fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d7cf0d0>]}
[0m13:08:19.705459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '39cf5fbf-a776-4982-8aac-db1de3138fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6fc610>]}
[0m13:08:19.705694 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:08:19.705907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '39cf5fbf-a776-4982-8aac-db1de3138fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d6fc640>]}
[0m13:08:19.707115 [info ] [MainThread]: 
[0m13:08:19.707620 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m13:08:19.708503 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m13:08:19.708738 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:08:19.708873 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:08:19.708986 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:08:20.816241 [debug] [ThreadPool]: SQL status: OK in 1.11 seconds
[0m13:08:20.824630 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m13:08:21.201896 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m13:08:21.215108 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:08:21.215295 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:08:21.215458 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m13:08:21.215598 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:08:22.175616 [debug] [ThreadPool]: SQL status: OK in 0.96 seconds
[0m13:08:22.184974 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:08:22.185171 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m13:08:25.864779 [debug] [ThreadPool]: SQL status: OK in 3.68 seconds
[0m13:08:25.870185 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m13:08:25.870374 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:08:25.870521 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m13:08:26.256561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '39cf5fbf-a776-4982-8aac-db1de3138fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d621790>]}
[0m13:08:26.257124 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:08:26.257325 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:08:26.257948 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:08:26.258264 [info ] [MainThread]: 
[0m13:08:26.261699 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m13:08:26.262014 [info ] [Thread-1  ]: 1 of 3 START sql table model default.ddl ....................................... [RUN]
[0m13:08:26.262747 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m13:08:26.262924 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m13:08:26.263104 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m13:08:26.266601 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m13:08:26.267040 [debug] [Thread-1  ]: finished collecting timing info
[0m13:08:26.267225 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m13:08:26.328897 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m13:08:26.329392 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:08:26.329519 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m13:08:26.329658 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select * from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m13:08:26.329767 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:08:50.237787 [debug] [Thread-1  ]: SQL status: OK in 23.91 seconds
[0m13:08:50.637105 [debug] [Thread-1  ]: finished collecting timing info
[0m13:08:50.637385 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m13:08:50.637540 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:08:50.637723 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m13:08:51.011507 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '39cf5fbf-a776-4982-8aac-db1de3138fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d8fcb80>]}
[0m13:08:51.012143 [info ] [Thread-1  ]: 1 of 3 OK created sql table model default.ddl .................................. [[32mOK[0m in 24.75s]
[0m13:08:51.012759 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m13:08:51.013074 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_first_dbt_model
[0m13:08:51.013445 [info ] [Thread-1  ]: 2 of 3 START sql table model default.my_first_dbt_model ........................ [RUN]
[0m13:08:51.014209 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_first_dbt_model"
[0m13:08:51.014408 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_first_dbt_model
[0m13:08:51.014589 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_first_dbt_model
[0m13:08:51.018109 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_first_dbt_model"
[0m13:08:51.018584 [debug] [Thread-1  ]: finished collecting timing info
[0m13:08:51.018759 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_first_dbt_model
[0m13:08:51.024755 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_first_dbt_model"
[0m13:08:51.025190 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:08:51.025348 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_first_dbt_model"
[0m13:08:51.025523 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_first_dbt_model"} */

  
    
        create or replace table hive_metastore.default.my_first_dbt_model
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:08:51.025666 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:08:58.356122 [debug] [Thread-1  ]: SQL status: OK in 7.33 seconds
[0m13:08:58.754130 [debug] [Thread-1  ]: finished collecting timing info
[0m13:08:58.754448 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: ROLLBACK
[0m13:08:58.754626 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:08:58.754789 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: Close
[0m13:08:59.140850 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '39cf5fbf-a776-4982-8aac-db1de3138fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d88aeb0>]}
[0m13:08:59.141505 [info ] [Thread-1  ]: 2 of 3 OK created sql table model default.my_first_dbt_model ................... [[32mOK[0m in 8.13s]
[0m13:08:59.142110 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_first_dbt_model
[0m13:08:59.142812 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_second_dbt_model
[0m13:08:59.143170 [info ] [Thread-1  ]: 3 of 3 START sql view model default.my_second_dbt_model ........................ [RUN]
[0m13:08:59.143940 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_second_dbt_model"
[0m13:08:59.144137 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_second_dbt_model
[0m13:08:59.144372 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_second_dbt_model
[0m13:08:59.147859 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_second_dbt_model"
[0m13:08:59.148283 [debug] [Thread-1  ]: finished collecting timing info
[0m13:08:59.148457 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_second_dbt_model
[0m13:08:59.167581 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_second_dbt_model"
[0m13:08:59.168050 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:08:59.168206 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_second_dbt_model"
[0m13:08:59.168362 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_second_dbt_model"} */
create or replace view hive_metastore.default.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from hive_metastore.default.my_first_dbt_model
where id = 1

[0m13:08:59.168506 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:09:00.403444 [debug] [Thread-1  ]: SQL status: OK in 1.23 seconds
[0m13:09:00.405712 [debug] [Thread-1  ]: finished collecting timing info
[0m13:09:00.405938 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: ROLLBACK
[0m13:09:00.406113 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:09:00.406274 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: Close
[0m13:09:00.782123 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '39cf5fbf-a776-4982-8aac-db1de3138fe1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d969d90>]}
[0m13:09:00.782778 [info ] [Thread-1  ]: 3 of 3 OK created sql view model default.my_second_dbt_model ................... [[32mOK[0m in 1.64s]
[0m13:09:00.783383 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_second_dbt_model
[0m13:09:00.784918 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m13:09:00.785142 [debug] [MainThread]: On master: ROLLBACK
[0m13:09:00.785312 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:09:01.185355 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:09:01.185712 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:09:01.185925 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:09:01.186149 [debug] [MainThread]: On master: ROLLBACK
[0m13:09:01.186345 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:09:01.186536 [debug] [MainThread]: On master: Close
[0m13:09:01.564728 [info ] [MainThread]: 
[0m13:09:01.565223 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 41.86 seconds (41.86s).
[0m13:09:01.565597 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:09:01.565794 [debug] [MainThread]: Connection 'model.ml_delivery.my_second_dbt_model' was properly closed.
[0m13:09:01.578333 [info ] [MainThread]: 
[0m13:09:01.578608 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:09:01.578887 [info ] [MainThread]: 
[0m13:09:01.579115 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m13:09:01.579426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d879790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d79e3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d83e070>]}
[0m13:09:01.579676 [debug] [MainThread]: Flushing usage events


============================== 2022-11-02 13:52:28.861639 | 5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64 ==============================
[0m13:52:28.861679 [info ] [MainThread]: Running with dbt=1.3.0
[0m13:52:28.862889 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:52:28.863109 [debug] [MainThread]: Tracking: tracking
[0m13:52:28.879239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1241fd2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1241fd460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1241fd400>]}
[0m13:52:28.982508 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:52:28.982903 [debug] [MainThread]: Partial parsing: updated file: ml_delivery://models/ddl.sql
[0m13:52:28.997236 [debug] [MainThread]: 1699: static parser successfully parsed ddl.sql
[0m13:52:29.030867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12439f0d0>]}
[0m13:52:29.040503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12437cfd0>]}
[0m13:52:29.040893 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m13:52:29.041186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12435d730>]}
[0m13:52:29.042647 [info ] [MainThread]: 
[0m13:52:29.043286 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m13:52:29.044298 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m13:52:29.044549 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m13:52:29.044713 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m13:52:29.044842 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:52:30.071855 [debug] [ThreadPool]: SQL status: OK in 1.03 seconds
[0m13:52:30.094913 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m13:52:30.451757 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m13:52:30.466757 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:30.466976 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:52:30.467137 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m13:52:30.467273 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:52:31.609878 [debug] [ThreadPool]: SQL status: OK in 1.14 seconds
[0m13:52:31.620148 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m13:52:31.620399 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m13:52:35.409435 [debug] [ThreadPool]: SQL status: OK in 3.79 seconds
[0m13:52:35.415407 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m13:52:35.415636 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m13:52:35.415768 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m13:52:35.819335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1242160a0>]}
[0m13:52:35.826802 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:35.827146 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:52:35.827781 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:52:35.828008 [info ] [MainThread]: 
[0m13:52:35.832011 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m13:52:35.832329 [info ] [Thread-1  ]: 1 of 3 START sql table model default.ddl ....................................... [RUN]
[0m13:52:35.833011 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m13:52:35.833235 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m13:52:35.833411 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m13:52:35.836882 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m13:52:35.837387 [debug] [Thread-1  ]: finished collecting timing info
[0m13:52:35.837607 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m13:52:35.912835 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m13:52:35.913493 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:35.913663 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m13:52:35.913834 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select *, to_date(date, 'yyyy-MM-dd') as New_Date from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m13:52:35.913976 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:52:43.018560 [debug] [Thread-1  ]: SQL status: OK in 7.1 seconds
[0m13:52:43.449900 [debug] [Thread-1  ]: finished collecting timing info
[0m13:52:43.450164 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m13:52:43.450307 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:52:43.450435 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m13:52:43.818613 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124540d90>]}
[0m13:52:43.819287 [info ] [Thread-1  ]: 1 of 3 OK created sql table model default.ddl .................................. [[32mOK[0m in 7.99s]
[0m13:52:43.819902 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m13:52:43.820202 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_first_dbt_model
[0m13:52:43.820641 [info ] [Thread-1  ]: 2 of 3 START sql table model default.my_first_dbt_model ........................ [RUN]
[0m13:52:43.821415 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_first_dbt_model"
[0m13:52:43.821614 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_first_dbt_model
[0m13:52:43.821797 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_first_dbt_model
[0m13:52:43.825408 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_first_dbt_model"
[0m13:52:43.825907 [debug] [Thread-1  ]: finished collecting timing info
[0m13:52:43.826082 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_first_dbt_model
[0m13:52:43.836027 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_first_dbt_model"
[0m13:52:43.836517 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:43.836718 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_first_dbt_model"
[0m13:52:43.836907 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_first_dbt_model"} */

  
    
        create or replace table hive_metastore.default.my_first_dbt_model
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:52:43.837048 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:52:47.661868 [debug] [Thread-1  ]: SQL status: OK in 3.82 seconds
[0m13:52:47.665735 [debug] [Thread-1  ]: finished collecting timing info
[0m13:52:47.665968 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: ROLLBACK
[0m13:52:47.666143 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:52:47.666304 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: Close
[0m13:52:48.039737 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124565e20>]}
[0m13:52:48.040374 [info ] [Thread-1  ]: 2 of 3 OK created sql table model default.my_first_dbt_model ................... [[32mOK[0m in 4.22s]
[0m13:52:48.040963 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_first_dbt_model
[0m13:52:48.041602 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_second_dbt_model
[0m13:52:48.041939 [info ] [Thread-1  ]: 3 of 3 START sql view model default.my_second_dbt_model ........................ [RUN]
[0m13:52:48.042858 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_second_dbt_model"
[0m13:52:48.043127 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_second_dbt_model
[0m13:52:48.043344 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_second_dbt_model
[0m13:52:48.048417 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_second_dbt_model"
[0m13:52:48.048947 [debug] [Thread-1  ]: finished collecting timing info
[0m13:52:48.049127 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_second_dbt_model
[0m13:52:48.067735 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_second_dbt_model"
[0m13:52:48.068248 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:48.068400 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_second_dbt_model"
[0m13:52:48.068553 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_second_dbt_model"} */
create or replace view hive_metastore.default.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from hive_metastore.default.my_first_dbt_model
where id = 1

[0m13:52:48.068687 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m13:52:50.109628 [debug] [Thread-1  ]: SQL status: OK in 2.04 seconds
[0m13:52:50.112588 [debug] [Thread-1  ]: finished collecting timing info
[0m13:52:50.112913 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: ROLLBACK
[0m13:52:50.113094 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m13:52:50.113242 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: Close
[0m13:52:50.479064 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5b06f25f-cfe3-4d7d-8a93-a4d8bc0d8c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124508040>]}
[0m13:52:50.479717 [info ] [Thread-1  ]: 3 of 3 OK created sql view model default.my_second_dbt_model ................... [[32mOK[0m in 2.44s]
[0m13:52:50.480327 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_second_dbt_model
[0m13:52:50.481901 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m13:52:50.482126 [debug] [MainThread]: On master: ROLLBACK
[0m13:52:50.482295 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:52:50.862196 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:50.862548 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m13:52:50.862760 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m13:52:50.862986 [debug] [MainThread]: On master: ROLLBACK
[0m13:52:50.863186 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m13:52:50.863377 [debug] [MainThread]: On master: Close
[0m13:52:51.225565 [info ] [MainThread]: 
[0m13:52:51.226080 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 22.18 seconds (22.18s).
[0m13:52:51.226456 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:52:51.226664 [debug] [MainThread]: Connection 'model.ml_delivery.my_second_dbt_model' was properly closed.
[0m13:52:51.241358 [info ] [MainThread]: 
[0m13:52:51.241664 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:52:51.241945 [info ] [MainThread]: 
[0m13:52:51.242179 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m13:52:51.242504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1245597c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12435d6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12458b0d0>]}
[0m13:52:51.242758 [debug] [MainThread]: Flushing usage events


============================== 2022-11-02 14:01:25.559186 | 440fcd0d-1e00-47fc-8c23-aa78fffa394e ==============================
[0m14:01:25.559221 [info ] [MainThread]: Running with dbt=1.3.0
[0m14:01:25.560001 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:01:25.560191 [debug] [MainThread]: Tracking: tracking
[0m14:01:25.574765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f479370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f4794f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f479490>]}
[0m14:01:25.689529 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:01:25.690245 [debug] [MainThread]: Partial parsing: updated file: ml_delivery://models/ddl.sql
[0m14:01:25.720101 [debug] [MainThread]: 1699: static parser successfully parsed ddl.sql
[0m14:01:25.769464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '440fcd0d-1e00-47fc-8c23-aa78fffa394e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f61f040>]}
[0m14:01:25.780285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '440fcd0d-1e00-47fc-8c23-aa78fffa394e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f5d9790>]}
[0m14:01:25.780770 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m14:01:25.781093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '440fcd0d-1e00-47fc-8c23-aa78fffa394e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091ee910>]}
[0m14:01:25.783112 [info ] [MainThread]: 
[0m14:01:25.784055 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:01:25.785420 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m14:01:25.785649 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m14:01:25.785815 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m14:01:25.786041 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:01:26.922238 [debug] [ThreadPool]: SQL status: OK in 1.14 seconds
[0m14:01:26.928851 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m14:01:27.317322 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m14:01:27.331960 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:27.332205 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m14:01:27.332368 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m14:01:27.332503 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:01:28.274942 [debug] [ThreadPool]: SQL status: OK in 0.94 seconds
[0m14:01:28.284539 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m14:01:28.284762 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m14:01:32.315625 [debug] [ThreadPool]: SQL status: OK in 4.03 seconds
[0m14:01:32.321287 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m14:01:32.321471 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m14:01:32.321616 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m14:01:32.686147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '440fcd0d-1e00-47fc-8c23-aa78fffa394e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f4929a0>]}
[0m14:01:32.686713 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:32.686897 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:01:32.687526 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:01:32.687844 [info ] [MainThread]: 
[0m14:01:32.694348 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m14:01:32.695137 [info ] [Thread-1  ]: 1 of 3 START sql table model default.ddl ....................................... [RUN]
[0m14:01:32.696166 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m14:01:32.696421 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m14:01:32.696686 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m14:01:32.700918 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m14:01:32.701463 [debug] [Thread-1  ]: finished collecting timing info
[0m14:01:32.701644 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m14:01:32.762976 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m14:01:32.763494 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:32.763632 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m14:01:32.763775 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m14:01:32.763897 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:01:37.518101 [debug] [Thread-1  ]: SQL status: OK in 4.75 seconds
[0m14:01:37.545671 [debug] [Thread-1  ]: finished collecting timing info
[0m14:01:37.545883 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m14:01:37.546024 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:01:37.546153 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m14:01:37.935562 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '440fcd0d-1e00-47fc-8c23-aa78fffa394e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f787b20>]}
[0m14:01:37.936223 [info ] [Thread-1  ]: 1 of 3 OK created sql table model default.ddl .................................. [[32mOK[0m in 5.24s]
[0m14:01:37.936852 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m14:01:37.937159 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_first_dbt_model
[0m14:01:37.937524 [info ] [Thread-1  ]: 2 of 3 START sql table model default.my_first_dbt_model ........................ [RUN]
[0m14:01:37.938293 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_first_dbt_model"
[0m14:01:37.938491 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_first_dbt_model
[0m14:01:37.938672 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_first_dbt_model
[0m14:01:37.942287 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_first_dbt_model"
[0m14:01:37.942736 [debug] [Thread-1  ]: finished collecting timing info
[0m14:01:37.942909 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_first_dbt_model
[0m14:01:37.952920 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_first_dbt_model"
[0m14:01:37.953409 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:37.953562 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_first_dbt_model"
[0m14:01:37.953730 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_first_dbt_model"} */

  
    
        create or replace table hive_metastore.default.my_first_dbt_model
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:01:37.953895 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:01:41.508229 [debug] [Thread-1  ]: SQL status: OK in 3.55 seconds
[0m14:01:41.512090 [debug] [Thread-1  ]: finished collecting timing info
[0m14:01:41.512319 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: ROLLBACK
[0m14:01:41.512492 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:01:41.512653 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: Close
[0m14:01:41.867125 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '440fcd0d-1e00-47fc-8c23-aa78fffa394e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f7dfe20>]}
[0m14:01:41.867758 [info ] [Thread-1  ]: 2 of 3 OK created sql table model default.my_first_dbt_model ................... [[32mOK[0m in 3.93s]
[0m14:01:41.868239 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_first_dbt_model
[0m14:01:41.868849 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_second_dbt_model
[0m14:01:41.869178 [info ] [Thread-1  ]: 3 of 3 START sql view model default.my_second_dbt_model ........................ [RUN]
[0m14:01:41.870034 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_second_dbt_model"
[0m14:01:41.870240 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_second_dbt_model
[0m14:01:41.870422 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_second_dbt_model
[0m14:01:41.875217 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_second_dbt_model"
[0m14:01:41.875710 [debug] [Thread-1  ]: finished collecting timing info
[0m14:01:41.875884 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_second_dbt_model
[0m14:01:41.894404 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_second_dbt_model"
[0m14:01:41.894908 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:41.895062 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_second_dbt_model"
[0m14:01:41.895214 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_second_dbt_model"} */
create or replace view hive_metastore.default.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from hive_metastore.default.my_first_dbt_model
where id = 1

[0m14:01:41.895348 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m14:01:43.631532 [debug] [Thread-1  ]: SQL status: OK in 1.74 seconds
[0m14:01:43.634519 [debug] [Thread-1  ]: finished collecting timing info
[0m14:01:43.634753 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: ROLLBACK
[0m14:01:43.634930 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m14:01:43.635092 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: Close
[0m14:01:43.996208 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '440fcd0d-1e00-47fc-8c23-aa78fffa394e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f7808b0>]}
[0m14:01:43.996863 [info ] [Thread-1  ]: 3 of 3 OK created sql view model default.my_second_dbt_model ................... [[32mOK[0m in 2.13s]
[0m14:01:43.997477 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_second_dbt_model
[0m14:01:43.999038 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m14:01:43.999264 [debug] [MainThread]: On master: ROLLBACK
[0m14:01:43.999433 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:01:44.368592 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:01:44.369009 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m14:01:44.369232 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m14:01:44.369457 [debug] [MainThread]: On master: ROLLBACK
[0m14:01:44.369654 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m14:01:44.369844 [debug] [MainThread]: On master: Close
[0m14:01:44.744690 [info ] [MainThread]: 
[0m14:01:44.745197 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 18.96 seconds (18.96s).
[0m14:01:44.745602 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:01:44.745836 [debug] [MainThread]: Connection 'model.ml_delivery.my_second_dbt_model' was properly closed.
[0m14:01:44.757440 [info ] [MainThread]: 
[0m14:01:44.757735 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:01:44.758000 [info ] [MainThread]: 
[0m14:01:44.758222 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m14:01:44.758507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091e90d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f5f7730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11f6fdd30>]}
[0m14:01:44.758739 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 21:49:06.421762 | 4151e611-0d69-4d91-bb96-0c124b6f24ac ==============================
[0m21:49:06.421819 [info ] [MainThread]: Running with dbt=1.3.0
[0m21:49:06.422902 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:49:06.423125 [debug] [MainThread]: Tracking: tracking
[0m21:49:06.465004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119635340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1196354c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119635460>]}
[0m21:49:06.512784 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m21:49:06.513172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11964e6d0>]}
[0m21:49:06.569640 [debug] [MainThread]: Parsing macros/statement.sql
[0m21:49:06.574099 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m21:49:06.582804 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:49:06.585397 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:49:06.606957 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:49:06.614663 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:49:06.615175 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:49:06.618711 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:49:06.636278 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m21:49:06.637710 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m21:49:06.644873 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:49:06.645793 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:49:06.646891 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:49:06.685954 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m21:49:06.688781 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m21:49:06.697094 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m21:49:06.697628 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m21:49:06.703095 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m21:49:06.725844 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m21:49:06.727658 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m21:49:06.732102 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m21:49:06.739756 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m21:49:06.746422 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:49:06.746922 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m21:49:06.748058 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:49:06.753015 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m21:49:06.753362 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:49:06.754927 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:49:06.767126 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:49:06.767546 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m21:49:06.767995 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:49:06.768405 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:49:06.769503 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m21:49:06.769971 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m21:49:06.770489 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:49:06.773758 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:49:06.775715 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:49:06.777122 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:49:06.790754 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:49:06.801846 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:49:06.811691 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:49:06.815493 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:49:06.817028 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:49:06.818527 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:49:06.825412 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:49:06.839039 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:49:06.840316 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m21:49:06.846207 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:49:06.855131 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:49:06.869857 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:49:06.874623 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:49:06.877784 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:49:06.882562 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:49:06.883670 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:49:06.886448 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:49:06.888391 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:49:06.894202 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:49:06.910060 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:49:06.911320 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:49:06.913482 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:49:06.914902 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:49:06.915651 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:49:06.916366 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:49:06.916946 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:49:06.918084 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:49:06.922593 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:49:06.930011 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:49:06.930782 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:49:06.931903 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:49:06.932792 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:49:06.933662 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:49:06.934775 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:49:06.935601 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:49:06.936689 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:49:06.937777 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:49:06.939844 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:49:06.940974 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:49:06.941981 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:49:06.942976 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:49:06.943919 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:49:06.944793 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:49:06.945791 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:49:06.946703 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:49:06.953087 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m21:49:06.954062 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:49:06.954926 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:49:06.956586 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:49:06.958576 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:49:06.959690 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m21:49:06.961084 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m21:49:06.962075 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:49:06.964048 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m21:49:06.967203 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:49:06.969737 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:49:06.983307 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:49:06.984943 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:49:06.996700 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:49:07.000351 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:49:07.006392 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:49:07.014638 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m21:49:07.021190 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:49:07.342414 [debug] [MainThread]: 1699: static parser successfully parsed ddl.sql
[0m21:49:07.355309 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m21:49:07.357967 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m21:49:07.423686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1197df0d0>]}
[0m21:49:07.432973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119840fd0>]}
[0m21:49:07.433304 [info ] [MainThread]: Found 3 models, 4 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m21:49:07.433569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119825910>]}
[0m21:49:07.434794 [info ] [MainThread]: 
[0m21:49:07.435311 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m21:49:07.436162 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m21:49:07.436374 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m21:49:07.436501 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m21:49:07.436620 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:49:09.992098 [debug] [ThreadPool]: SQL status: OK in 2.56 seconds
[0m21:49:10.031003 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m21:49:10.598278 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m21:49:10.611228 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:10.611398 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m21:49:10.611559 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m21:49:10.611694 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:49:12.644437 [debug] [ThreadPool]: SQL status: OK in 2.03 seconds
[0m21:49:12.654216 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m21:49:12.654427 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m21:49:22.534655 [debug] [ThreadPool]: SQL status: OK in 9.88 seconds
[0m21:49:22.993333 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m21:49:22.993710 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m21:49:22.993931 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m21:49:23.365694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11964eaf0>]}
[0m21:49:23.366158 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:23.366343 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:49:23.366962 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:49:23.367277 [info ] [MainThread]: 
[0m21:49:23.372403 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m21:49:23.372739 [info ] [Thread-1  ]: 1 of 3 START sql table model default.ddl ....................................... [RUN]
[0m21:49:23.373438 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m21:49:23.373618 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m21:49:23.373793 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m21:49:23.380767 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m21:49:23.382555 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:23.382761 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m21:49:23.446942 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m21:49:23.447520 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:23.447705 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m21:49:23.447857 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m21:49:23.447983 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:29.130774 [debug] [Thread-1  ]: SQL status: OK in 5.68 seconds
[0m21:49:29.158229 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:29.158431 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m21:49:29.158577 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:49:29.158710 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m21:49:29.543597 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119953460>]}
[0m21:49:29.544286 [info ] [Thread-1  ]: 1 of 3 OK created sql table model default.ddl .................................. [[32mOK[0m in 6.17s]
[0m21:49:29.544901 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m21:49:29.545227 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_first_dbt_model
[0m21:49:29.545650 [info ] [Thread-1  ]: 2 of 3 START sql table model default.my_first_dbt_model ........................ [RUN]
[0m21:49:29.546420 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_first_dbt_model"
[0m21:49:29.546622 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_first_dbt_model
[0m21:49:29.546805 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_first_dbt_model
[0m21:49:29.550408 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_first_dbt_model"
[0m21:49:29.550869 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:29.551043 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_first_dbt_model
[0m21:49:29.557402 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_first_dbt_model"
[0m21:49:29.557881 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:29.558042 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_first_dbt_model"
[0m21:49:29.558217 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_first_dbt_model"} */

  
    
        create or replace table hive_metastore.default.my_first_dbt_model
      
      
    using delta
      
      
      
      
      
      
      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:49:29.558358 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:33.806170 [debug] [Thread-1  ]: SQL status: OK in 4.25 seconds
[0m21:49:33.810059 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:33.810283 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: ROLLBACK
[0m21:49:33.810548 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:49:33.810719 [debug] [Thread-1  ]: On model.ml_delivery.my_first_dbt_model: Close
[0m21:49:34.206404 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119909ac0>]}
[0m21:49:34.207063 [info ] [Thread-1  ]: 2 of 3 OK created sql table model default.my_first_dbt_model ................... [[32mOK[0m in 4.66s]
[0m21:49:34.207660 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_first_dbt_model
[0m21:49:34.208385 [debug] [Thread-1  ]: Began running node model.ml_delivery.my_second_dbt_model
[0m21:49:34.208757 [info ] [Thread-1  ]: 3 of 3 START sql view model default.my_second_dbt_model ........................ [RUN]
[0m21:49:34.209517 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.my_second_dbt_model"
[0m21:49:34.209711 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.my_second_dbt_model
[0m21:49:34.209890 [debug] [Thread-1  ]: Compiling model.ml_delivery.my_second_dbt_model
[0m21:49:34.214895 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.my_second_dbt_model"
[0m21:49:34.216974 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:34.217338 [debug] [Thread-1  ]: Began executing node model.ml_delivery.my_second_dbt_model
[0m21:49:34.239832 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.my_second_dbt_model"
[0m21:49:34.241781 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:34.242033 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.my_second_dbt_model"
[0m21:49:34.242221 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.my_second_dbt_model"} */
create or replace view hive_metastore.default.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from hive_metastore.default.my_first_dbt_model
where id = 1

[0m21:49:34.242361 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:49:36.428891 [debug] [Thread-1  ]: SQL status: OK in 2.19 seconds
[0m21:49:36.431862 [debug] [Thread-1  ]: finished collecting timing info
[0m21:49:36.432088 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: ROLLBACK
[0m21:49:36.432270 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m21:49:36.432435 [debug] [Thread-1  ]: On model.ml_delivery.my_second_dbt_model: Close
[0m21:49:36.816909 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4151e611-0d69-4d91-bb96-0c124b6f24ac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199e00d0>]}
[0m21:49:36.817438 [info ] [Thread-1  ]: 3 of 3 OK created sql view model default.my_second_dbt_model ................... [[32mOK[0m in 2.61s]
[0m21:49:36.817921 [debug] [Thread-1  ]: Finished running node model.ml_delivery.my_second_dbt_model
[0m21:49:36.819315 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m21:49:36.819545 [debug] [MainThread]: On master: ROLLBACK
[0m21:49:36.819720 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:49:37.223810 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:49:37.224200 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m21:49:37.224422 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m21:49:37.224652 [debug] [MainThread]: On master: ROLLBACK
[0m21:49:37.224856 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m21:49:37.225050 [debug] [MainThread]: On master: Close
[0m21:49:37.602684 [info ] [MainThread]: 
[0m21:49:37.603298 [info ] [MainThread]: Finished running 2 table models, 1 view model in 0 hours 0 minutes and 30.17 seconds (30.17s).
[0m21:49:37.603677 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:49:37.603881 [debug] [MainThread]: Connection 'model.ml_delivery.my_second_dbt_model' was properly closed.
[0m21:49:37.616201 [info ] [MainThread]: 
[0m21:49:37.616477 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:49:37.616761 [info ] [MainThread]: 
[0m21:49:37.616988 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m21:49:37.617299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1198e5a60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11964ecd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199c3220>]}
[0m21:49:37.617551 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 22:18:13.634333 | d009c4f8-fe14-4840-9f1b-456980573358 ==============================
[0m22:18:13.634444 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:18:13.635656 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:18:13.635858 [debug] [MainThread]: Tracking: tracking
[0m22:18:13.656002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fbc1310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fbc1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fbc1430>]}
[0m22:18:13.764496 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 1 files added, 0 files changed.
[0m22:18:13.764931 [debug] [MainThread]: Partial parsing: added file: ml_delivery://models/data_profiling.py
[0m22:18:13.765209 [debug] [MainThread]: Partial parsing: deleted file: ml_delivery://models/example/my_first_dbt_model.sql
[0m22:18:13.765369 [debug] [MainThread]: Partial parsing: deleted file: ml_delivery://models/example/my_second_dbt_model.sql
[0m22:18:13.814987 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.ml_delivery.example

[0m22:18:13.820783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd009c4f8-fe14-4840-9f1b-456980573358', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd47fd0>]}
[0m22:18:13.830067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd009c4f8-fe14-4840-9f1b-456980573358', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd217f0>]}
[0m22:18:13.830423 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:18:13.830670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd009c4f8-fe14-4840-9f1b-456980573358', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd218b0>]}
[0m22:18:13.831946 [info ] [MainThread]: 
[0m22:18:13.832628 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:18:13.833635 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m22:18:13.833895 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:18:13.834062 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:18:13.834232 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:18:15.101044 [debug] [ThreadPool]: SQL status: OK in 1.27 seconds
[0m22:18:15.127802 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m22:18:15.616972 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m22:18:15.631729 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:15.631913 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:18:15.632073 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m22:18:15.632211 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:18:16.590883 [debug] [ThreadPool]: SQL status: OK in 0.96 seconds
[0m22:18:16.600624 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:18:16.600833 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m22:18:20.447286 [debug] [ThreadPool]: SQL status: OK in 3.85 seconds
[0m22:18:20.455699 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m22:18:20.455977 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:18:20.456142 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m22:18:20.882115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd009c4f8-fe14-4840-9f1b-456980573358', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fccea60>]}
[0m22:18:20.882607 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:20.882786 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:18:20.883406 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:18:20.883716 [info ] [MainThread]: 
[0m22:18:20.889868 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m22:18:20.890293 [info ] [Thread-1  ]: 1 of 2 START sql table model default.ddl ....................................... [RUN]
[0m22:18:20.891143 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m22:18:20.891329 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m22:18:20.891507 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m22:18:20.894991 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m22:18:20.895466 [debug] [Thread-1  ]: finished collecting timing info
[0m22:18:20.895653 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m22:18:20.955523 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m22:18:20.956003 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:20.956116 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m22:18:20.956232 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m22:18:20.956331 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:18:25.513632 [debug] [Thread-1  ]: SQL status: OK in 4.56 seconds
[0m22:18:25.542439 [debug] [Thread-1  ]: finished collecting timing info
[0m22:18:25.542705 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m22:18:25.542865 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:18:25.543004 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m22:18:25.934484 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd009c4f8-fe14-4840-9f1b-456980573358', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fe8ef70>]}
[0m22:18:25.935042 [info ] [Thread-1  ]: 1 of 2 OK created sql table model default.ddl .................................. [[32mOK[0m in 5.04s]
[0m22:18:25.935565 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m22:18:25.936176 [debug] [Thread-1  ]: Began running node model.ml_delivery.data_profiling
[0m22:18:25.936545 [info ] [Thread-1  ]: 2 of 2 START python view model default.data_profiling .......................... [RUN]
[0m22:18:25.937335 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.data_profiling"
[0m22:18:25.937536 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.data_profiling
[0m22:18:25.937733 [debug] [Thread-1  ]: Compiling model.ml_delivery.data_profiling
[0m22:18:25.964150 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.data_profiling"
[0m22:18:25.964644 [debug] [Thread-1  ]: finished collecting timing info
[0m22:18:25.964819 [debug] [Thread-1  ]: Began executing node model.ml_delivery.data_profiling
[0m22:18:25.966731 [debug] [Thread-1  ]: finished collecting timing info
[0m22:18:25.967308 [debug] [Thread-1  ]: Runtime Error in model data_profiling (models/data_profiling.py)
  Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:18:25.967540 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd009c4f8-fe14-4840-9f1b-456980573358', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fecd220>]}
[0m22:18:25.967877 [error] [Thread-1  ]: 2 of 2 ERROR creating python view model default.data_profiling ................. [[31mERROR[0m in 0.03s]
[0m22:18:25.968244 [debug] [Thread-1  ]: Finished running node model.ml_delivery.data_profiling
[0m22:18:25.969395 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:18:25.969584 [debug] [MainThread]: On master: ROLLBACK
[0m22:18:25.969727 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:18:26.406020 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:18:26.406303 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:18:26.406527 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:18:26.406678 [debug] [MainThread]: On master: ROLLBACK
[0m22:18:26.406804 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:18:26.406923 [debug] [MainThread]: On master: Close
[0m22:18:26.836193 [info ] [MainThread]: 
[0m22:18:26.836479 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 13.00 seconds (13.00s).
[0m22:18:26.836672 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:18:26.836772 [debug] [MainThread]: Connection 'model.ml_delivery.data_profiling' was properly closed.
[0m22:18:26.846146 [info ] [MainThread]: 
[0m22:18:26.846596 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:18:26.846955 [info ] [MainThread]: 
[0m22:18:26.847168 [error] [MainThread]: [33mRuntime Error in model data_profiling (models/data_profiling.py)[0m
[0m22:18:26.847468 [error] [MainThread]:   Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:18:26.847758 [info ] [MainThread]: 
[0m22:18:26.847928 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m22:18:26.848360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fee90a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fd218e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11fe8e160>]}
[0m22:18:26.848580 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 22:19:53.172264 | b903cfa2-31db-45e1-94d2-da7944dc57fe ==============================
[0m22:19:53.172384 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:19:53.173737 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:19:53.173928 [debug] [MainThread]: Tracking: tracking
[0m22:19:53.192207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12397b2b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12397b430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12397b3d0>]}
[0m22:19:53.295974 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:19:53.296478 [debug] [MainThread]: Partial parsing: updated file: ml_delivery://models/data_profiling.py
[0m22:19:53.315373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123a3e640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b25790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x123b258e0>]}
[0m22:19:53.315713 [debug] [MainThread]: Flushing usage events
[0m22:19:53.582882 [error] [MainThread]: Encountered an error:
Parsing Error in model data_profiling (models/data_profiling.py)
  No jinja in python model code is allowed


============================== 2022-11-07 22:20:30.256831 | bfd9dd8e-8c86-4dfa-ae98-9911cff9e62f ==============================
[0m22:20:30.256881 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:20:30.257779 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:20:30.258004 [debug] [MainThread]: Tracking: tracking
[0m22:20:30.272136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118526310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118526490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118526430>]}
[0m22:20:30.338373 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:20:30.338723 [debug] [MainThread]: Partial parsing: updated file: ml_delivery://models/data_profiling.py
[0m22:20:30.378876 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.ml_delivery.example

[0m22:20:30.384463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bfd9dd8e-8c86-4dfa-ae98-9911cff9e62f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186be0d0>]}
[0m22:20:30.395323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bfd9dd8e-8c86-4dfa-ae98-9911cff9e62f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1185493d0>]}
[0m22:20:30.395734 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:20:30.396014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bfd9dd8e-8c86-4dfa-ae98-9911cff9e62f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118549340>]}
[0m22:20:30.397396 [info ] [MainThread]: 
[0m22:20:30.398099 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:20:30.399143 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m22:20:30.399425 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:20:30.399607 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:20:30.399756 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:20:31.607554 [debug] [ThreadPool]: SQL status: OK in 1.21 seconds
[0m22:20:31.648789 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m22:20:32.068683 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m22:20:32.082253 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:32.082418 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:20:32.082548 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m22:20:32.082661 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:20:32.968333 [debug] [ThreadPool]: SQL status: OK in 0.89 seconds
[0m22:20:32.978144 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:20:32.978355 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m22:20:37.066297 [debug] [ThreadPool]: SQL status: OK in 4.09 seconds
[0m22:20:37.074879 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m22:20:37.075117 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:20:37.075275 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m22:20:37.451360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bfd9dd8e-8c86-4dfa-ae98-9911cff9e62f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1186f54f0>]}
[0m22:20:37.451903 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:37.452135 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:20:37.452916 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:20:37.453313 [info ] [MainThread]: 
[0m22:20:37.459769 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m22:20:37.460192 [info ] [Thread-1  ]: 1 of 2 START sql table model default.ddl ....................................... [RUN]
[0m22:20:37.461045 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m22:20:37.461222 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m22:20:37.461409 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m22:20:37.464950 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m22:20:37.465419 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:37.465650 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m22:20:37.527910 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m22:20:37.528371 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:37.528488 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m22:20:37.528607 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m22:20:37.528712 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:20:41.055441 [debug] [Thread-1  ]: SQL status: OK in 3.53 seconds
[0m22:20:41.096615 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:41.096921 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m22:20:41.097081 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:20:41.097222 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m22:20:41.478324 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bfd9dd8e-8c86-4dfa-ae98-9911cff9e62f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187eec40>]}
[0m22:20:41.478846 [info ] [Thread-1  ]: 1 of 2 OK created sql table model default.ddl .................................. [[32mOK[0m in 4.02s]
[0m22:20:41.479363 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m22:20:41.480045 [debug] [Thread-1  ]: Began running node model.ml_delivery.data_profiling
[0m22:20:41.480401 [info ] [Thread-1  ]: 2 of 2 START python view model default.data_profiling .......................... [RUN]
[0m22:20:41.481107 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.data_profiling"
[0m22:20:41.481290 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.data_profiling
[0m22:20:41.481444 [debug] [Thread-1  ]: Compiling model.ml_delivery.data_profiling
[0m22:20:41.507114 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.data_profiling"
[0m22:20:41.507683 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:41.507863 [debug] [Thread-1  ]: Began executing node model.ml_delivery.data_profiling
[0m22:20:41.509748 [debug] [Thread-1  ]: finished collecting timing info
[0m22:20:41.510293 [debug] [Thread-1  ]: Runtime Error in model data_profiling (models/data_profiling.py)
  Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:20:41.510509 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bfd9dd8e-8c86-4dfa-ae98-9911cff9e62f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11888bcd0>]}
[0m22:20:41.510815 [error] [Thread-1  ]: 2 of 2 ERROR creating python view model default.data_profiling ................. [[31mERROR[0m in 0.03s]
[0m22:20:41.511148 [debug] [Thread-1  ]: Finished running node model.ml_delivery.data_profiling
[0m22:20:41.512305 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:20:41.512472 [debug] [MainThread]: On master: ROLLBACK
[0m22:20:41.512599 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:41.926838 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:20:41.927329 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:20:41.927570 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:20:41.927802 [debug] [MainThread]: On master: ROLLBACK
[0m22:20:41.928099 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:20:41.928303 [debug] [MainThread]: On master: Close
[0m22:20:42.329988 [info ] [MainThread]: 
[0m22:20:42.330516 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 11.93 seconds (11.93s).
[0m22:20:42.330901 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:20:42.331100 [debug] [MainThread]: Connection 'model.ml_delivery.data_profiling' was properly closed.
[0m22:20:42.341704 [info ] [MainThread]: 
[0m22:20:42.341972 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:20:42.342236 [info ] [MainThread]: 
[0m22:20:42.342468 [error] [MainThread]: [33mRuntime Error in model data_profiling (models/data_profiling.py)[0m
[0m22:20:42.342685 [error] [MainThread]:   Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:20:42.342903 [info ] [MainThread]: 
[0m22:20:42.343122 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m22:20:42.343431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11888bcd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187b69a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1187ee730>]}
[0m22:20:42.343686 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 22:21:33.677992 | 48bc55ff-f061-4a86-945f-3b8698f71cfe ==============================
[0m22:21:33.678050 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:21:33.678865 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:21:33.679168 [debug] [MainThread]: Tracking: tracking
[0m22:21:33.693687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a932b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a93430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118a933d0>]}
[0m22:21:33.736659 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m22:21:33.736951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '48bc55ff-f061-4a86-945f-3b8698f71cfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118aae070>]}
[0m22:21:33.756113 [debug] [MainThread]: Parsing macros/statement.sql
[0m22:21:33.758658 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m22:21:33.765378 [debug] [MainThread]: Parsing macros/catalog.sql
[0m22:21:33.767470 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:21:33.788310 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:21:33.796262 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:21:33.796791 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:21:33.800365 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:21:33.818046 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:21:33.819483 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:21:33.826502 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:21:33.827406 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:21:33.828431 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:21:33.866489 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m22:21:33.869242 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:21:33.877377 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:21:33.877854 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:21:33.883189 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:21:33.906790 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m22:21:33.908775 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m22:21:33.913158 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:21:33.920588 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:21:33.926963 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:21:33.927396 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m22:21:33.928467 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:21:33.933051 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m22:21:33.933373 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:21:33.934822 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:21:33.946396 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:21:33.946829 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:21:33.947294 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:21:33.947717 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:21:33.948868 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:21:33.949338 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:21:33.949863 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m22:21:33.953167 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m22:21:33.955181 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m22:21:33.956576 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m22:21:33.970782 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m22:21:33.982647 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m22:21:33.993393 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m22:21:33.997333 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m22:21:33.998883 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m22:21:34.000475 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m22:21:34.007642 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m22:21:34.020435 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m22:21:34.021636 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m22:21:34.027262 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m22:21:34.035787 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m22:21:34.050201 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m22:21:34.054849 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m22:21:34.057868 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m22:21:34.062515 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m22:21:34.063634 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m22:21:34.066478 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m22:21:34.068474 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m22:21:34.074478 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m22:21:34.090834 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m22:21:34.092131 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m22:21:34.094333 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m22:21:34.095665 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m22:21:34.096427 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m22:21:34.097111 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m22:21:34.097716 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m22:21:34.098893 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m22:21:34.103567 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m22:21:34.110955 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m22:21:34.111701 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m22:21:34.112801 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:21:34.113677 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m22:21:34.114514 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:21:34.115606 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m22:21:34.116338 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m22:21:34.117255 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m22:21:34.118334 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:21:34.120331 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:21:34.121451 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m22:21:34.122429 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m22:21:34.123396 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m22:21:34.124348 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:21:34.125194 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m22:21:34.126173 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m22:21:34.126990 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m22:21:34.133034 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:21:34.133983 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:21:34.134809 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m22:21:34.136460 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:21:34.138524 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m22:21:34.139441 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:21:34.140780 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:21:34.141761 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m22:21:34.143785 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m22:21:34.146831 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m22:21:34.149337 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m22:21:34.162703 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m22:21:34.164442 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m22:21:34.176858 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m22:21:34.180804 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m22:21:34.187359 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m22:21:34.196213 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m22:21:34.201463 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m22:21:34.528698 [debug] [MainThread]: 1699: static parser successfully parsed ddl.sql
[0m22:21:34.585415 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.ml_delivery.models

[0m22:21:34.589876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '48bc55ff-f061-4a86-945f-3b8698f71cfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118cc90d0>]}
[0m22:21:34.596142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '48bc55ff-f061-4a86-945f-3b8698f71cfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118bf9c10>]}
[0m22:21:34.596616 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:21:34.596825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48bc55ff-f061-4a86-945f-3b8698f71cfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c8fd00>]}
[0m22:21:34.597835 [info ] [MainThread]: 
[0m22:21:34.598324 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:21:34.599171 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m22:21:34.599378 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:21:34.599507 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:21:34.599612 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:21:35.500962 [debug] [ThreadPool]: SQL status: OK in 0.9 seconds
[0m22:21:35.507701 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m22:21:35.879438 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m22:21:35.893784 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:35.893996 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:21:35.894153 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m22:21:35.894288 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:21:36.733241 [debug] [ThreadPool]: SQL status: OK in 0.84 seconds
[0m22:21:36.742618 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:21:36.742816 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m22:21:40.702393 [debug] [ThreadPool]: SQL status: OK in 3.96 seconds
[0m22:21:40.708515 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m22:21:40.708745 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:21:40.708894 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m22:21:41.096282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '48bc55ff-f061-4a86-945f-3b8698f71cfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118c840a0>]}
[0m22:21:41.096832 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:41.097063 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:21:41.097910 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:21:41.098326 [info ] [MainThread]: 
[0m22:21:41.104626 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m22:21:41.105054 [info ] [Thread-1  ]: 1 of 2 START sql table model default.ddl ....................................... [RUN]
[0m22:21:41.105880 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m22:21:41.106102 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m22:21:41.106284 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m22:21:41.109753 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m22:21:41.110231 [debug] [Thread-1  ]: finished collecting timing info
[0m22:21:41.110422 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m22:21:41.172781 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m22:21:41.173273 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:41.173393 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m22:21:41.173517 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m22:21:41.173626 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:21:44.654809 [debug] [Thread-1  ]: SQL status: OK in 3.48 seconds
[0m22:21:44.687647 [debug] [Thread-1  ]: finished collecting timing info
[0m22:21:44.687897 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m22:21:44.688053 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:21:44.688196 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m22:21:45.065822 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48bc55ff-f061-4a86-945f-3b8698f71cfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118db3e50>]}
[0m22:21:45.066500 [info ] [Thread-1  ]: 1 of 2 OK created sql table model default.ddl .................................. [[32mOK[0m in 3.96s]
[0m22:21:45.067119 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m22:21:45.067831 [debug] [Thread-1  ]: Began running node model.ml_delivery.data_profiling
[0m22:21:45.068190 [info ] [Thread-1  ]: 2 of 2 START python view model default.data_profiling .......................... [RUN]
[0m22:21:45.068992 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.data_profiling"
[0m22:21:45.069186 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.data_profiling
[0m22:21:45.069364 [debug] [Thread-1  ]: Compiling model.ml_delivery.data_profiling
[0m22:21:45.094085 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.data_profiling"
[0m22:21:45.094489 [debug] [Thread-1  ]: finished collecting timing info
[0m22:21:45.094646 [debug] [Thread-1  ]: Began executing node model.ml_delivery.data_profiling
[0m22:21:45.096339 [debug] [Thread-1  ]: finished collecting timing info
[0m22:21:45.096842 [debug] [Thread-1  ]: Runtime Error in model data_profiling (models/data_profiling.py)
  Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:21:45.097047 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '48bc55ff-f061-4a86-945f-3b8698f71cfe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118de4be0>]}
[0m22:21:45.097343 [error] [Thread-1  ]: 2 of 2 ERROR creating python view model default.data_profiling ................. [[31mERROR[0m in 0.03s]
[0m22:21:45.097669 [debug] [Thread-1  ]: Finished running node model.ml_delivery.data_profiling
[0m22:21:45.098688 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:21:45.098860 [debug] [MainThread]: On master: ROLLBACK
[0m22:21:45.098989 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:21:45.504790 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:21:45.505164 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:21:45.505377 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:21:45.505600 [debug] [MainThread]: On master: ROLLBACK
[0m22:21:45.505801 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:21:45.505994 [debug] [MainThread]: On master: Close
[0m22:21:45.877966 [info ] [MainThread]: 
[0m22:21:45.896779 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 11.28 seconds (11.28s).
[0m22:21:45.897525 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:21:45.899284 [debug] [MainThread]: Connection 'model.ml_delivery.data_profiling' was properly closed.
[0m22:21:45.922536 [info ] [MainThread]: 
[0m22:21:45.922991 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:21:45.923395 [info ] [MainThread]: 
[0m22:21:45.923738 [error] [MainThread]: [33mRuntime Error in model data_profiling (models/data_profiling.py)[0m
[0m22:21:45.924066 [error] [MainThread]:   Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:21:45.924343 [info ] [MainThread]: 
[0m22:21:45.924600 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m22:21:45.924957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118de4820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118ca4370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118d4b4c0>]}
[0m22:21:45.925253 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 22:22:05.950056 | e1618c62-da41-4912-aa2c-f2a131fd1abf ==============================
[0m22:22:05.950117 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:22:05.951915 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:22:05.952146 [debug] [MainThread]: Tracking: tracking
[0m22:22:05.965985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ef02e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ef0460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119ef0400>]}
[0m22:22:06.011364 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m22:22:06.011721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e1618c62-da41-4912-aa2c-f2a131fd1abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119f0a0d0>]}
[0m22:22:06.065219 [debug] [MainThread]: Parsing macros/statement.sql
[0m22:22:06.069359 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m22:22:06.078094 [debug] [MainThread]: Parsing macros/catalog.sql
[0m22:22:06.080933 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:22:06.104087 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:22:06.112037 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:22:06.112556 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:22:06.116274 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:22:06.134184 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:22:06.135630 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:22:06.142950 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:22:06.143886 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:22:06.144949 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:22:06.185407 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m22:22:06.188449 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:22:06.198470 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:22:06.199061 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:22:06.204893 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:22:06.228462 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m22:22:06.230416 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m22:22:06.235061 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:22:06.242989 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:22:06.249828 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:22:06.250349 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m22:22:06.251485 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:22:06.256470 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m22:22:06.256810 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:22:06.258404 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:22:06.270223 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:22:06.270656 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:22:06.271126 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:22:06.271553 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:22:06.272717 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:22:06.273206 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:22:06.273751 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m22:22:06.277312 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m22:22:06.279373 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m22:22:06.280847 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m22:22:06.295308 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m22:22:06.307004 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m22:22:06.317587 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m22:22:06.321577 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m22:22:06.323116 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m22:22:06.324696 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m22:22:06.331687 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m22:22:06.345280 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m22:22:06.346561 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m22:22:06.352607 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m22:22:06.361658 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m22:22:06.376197 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m22:22:06.380917 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m22:22:06.384024 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m22:22:06.388829 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m22:22:06.389960 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m22:22:06.392917 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m22:22:06.394927 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m22:22:06.400981 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m22:22:06.417121 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m22:22:06.418395 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m22:22:06.420561 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m22:22:06.422001 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m22:22:06.422785 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m22:22:06.423487 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m22:22:06.424080 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m22:22:06.425359 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m22:22:06.429938 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m22:22:06.437312 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m22:22:06.438051 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m22:22:06.439123 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:22:06.439981 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m22:22:06.440982 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:22:06.442084 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m22:22:06.442829 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m22:22:06.443757 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m22:22:06.444873 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:22:06.446872 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:22:06.448045 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m22:22:06.449007 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m22:22:06.449951 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m22:22:06.450880 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:22:06.451707 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m22:22:06.452749 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m22:22:06.453580 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m22:22:06.459572 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:22:06.460655 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:22:06.461591 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m22:22:06.463190 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:22:06.465317 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m22:22:06.466240 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:22:06.467550 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:22:06.468481 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m22:22:06.470386 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m22:22:06.473386 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m22:22:06.475826 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m22:22:06.488809 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m22:22:06.490516 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m22:22:06.502800 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m22:22:06.506749 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m22:22:06.513241 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m22:22:06.522086 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m22:22:06.527385 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m22:22:06.854227 [debug] [MainThread]: 1699: static parser successfully parsed ddl.sql
[0m22:22:06.909177 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e1618c62-da41-4912-aa2c-f2a131fd1abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0910d0>]}
[0m22:22:06.916579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e1618c62-da41-4912-aa2c-f2a131fd1abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a025b50>]}
[0m22:22:06.916843 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:22:06.917037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1618c62-da41-4912-aa2c-f2a131fd1abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0ebd30>]}
[0m22:22:06.918021 [info ] [MainThread]: 
[0m22:22:06.918524 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:22:06.919267 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m22:22:06.919480 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:22:06.919614 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:22:06.919722 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:22:07.870910 [debug] [ThreadPool]: SQL status: OK in 0.95 seconds
[0m22:22:07.878379 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m22:22:08.272554 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m22:22:08.290982 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:22:08.297928 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:22:08.298750 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m22:22:08.299165 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:22:09.122562 [debug] [ThreadPool]: SQL status: OK in 0.82 seconds
[0m22:22:09.132045 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:22:09.132236 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m22:22:13.107507 [debug] [ThreadPool]: SQL status: OK in 3.98 seconds
[0m22:22:13.113263 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m22:22:13.113448 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:22:13.113600 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m22:22:13.511766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e1618c62-da41-4912-aa2c-f2a131fd1abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a0ea070>]}
[0m22:22:13.512318 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:22:13.512509 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:22:13.513138 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:22:13.513458 [info ] [MainThread]: 
[0m22:22:13.519548 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m22:22:13.519970 [info ] [Thread-1  ]: 1 of 2 START sql table model default.ddl ....................................... [RUN]
[0m22:22:13.520804 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m22:22:13.520983 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m22:22:13.521161 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m22:22:13.524634 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m22:22:13.525083 [debug] [Thread-1  ]: finished collecting timing info
[0m22:22:13.525270 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m22:22:13.588177 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m22:22:13.588666 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:22:13.588788 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m22:22:13.588912 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m22:22:13.589043 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:22:17.112382 [debug] [Thread-1  ]: SQL status: OK in 3.52 seconds
[0m22:22:17.141066 [debug] [Thread-1  ]: finished collecting timing info
[0m22:22:17.141306 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m22:22:17.141457 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:22:17.141593 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m22:22:17.540980 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1618c62-da41-4912-aa2c-f2a131fd1abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a21de20>]}
[0m22:22:17.541638 [info ] [Thread-1  ]: 1 of 2 OK created sql table model default.ddl .................................. [[32mOK[0m in 4.02s]
[0m22:22:17.542268 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m22:22:17.543076 [debug] [Thread-1  ]: Began running node model.ml_delivery.data_profiling
[0m22:22:17.543545 [info ] [Thread-1  ]: 2 of 2 START python view model default.data_profiling .......................... [RUN]
[0m22:22:17.544628 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.data_profiling"
[0m22:22:17.544999 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.data_profiling
[0m22:22:17.545222 [debug] [Thread-1  ]: Compiling model.ml_delivery.data_profiling
[0m22:22:17.571731 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.data_profiling"
[0m22:22:17.572269 [debug] [Thread-1  ]: finished collecting timing info
[0m22:22:17.572457 [debug] [Thread-1  ]: Began executing node model.ml_delivery.data_profiling
[0m22:22:17.574510 [debug] [Thread-1  ]: finished collecting timing info
[0m22:22:17.575089 [debug] [Thread-1  ]: Runtime Error in model data_profiling (models/data_profiling.py)
  Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:22:17.575384 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e1618c62-da41-4912-aa2c-f2a131fd1abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a2746a0>]}
[0m22:22:17.575850 [error] [Thread-1  ]: 2 of 2 ERROR creating python view model default.data_profiling ................. [[31mERROR[0m in 0.03s]
[0m22:22:17.576258 [debug] [Thread-1  ]: Finished running node model.ml_delivery.data_profiling
[0m22:22:17.577542 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:22:17.577732 [debug] [MainThread]: On master: ROLLBACK
[0m22:22:17.577874 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:22:17.974352 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:22:17.974776 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:22:17.975000 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:22:17.975227 [debug] [MainThread]: On master: ROLLBACK
[0m22:22:17.975428 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:22:17.975624 [debug] [MainThread]: On master: Close
[0m22:22:18.359377 [info ] [MainThread]: 
[0m22:22:18.359914 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 11.44 seconds (11.44s).
[0m22:22:18.360292 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:22:18.360498 [debug] [MainThread]: Connection 'model.ml_delivery.data_profiling' was properly closed.
[0m22:22:18.371183 [info ] [MainThread]: 
[0m22:22:18.371465 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:22:18.371736 [info ] [MainThread]: 
[0m22:22:18.371969 [error] [MainThread]: [33mRuntime Error in model data_profiling (models/data_profiling.py)[0m
[0m22:22:18.372189 [error] [MainThread]:   Materialization "materialization_view_databricks" only supports languages ['sql']; got "python"
[0m22:22:18.372410 [info ] [MainThread]: 
[0m22:22:18.372629 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m22:22:18.372935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a274820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a191df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11a1a73a0>]}
[0m22:22:18.373196 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 22:24:13.269761 | 0ac92a5e-0b11-478c-ae19-5df69cd94cfb ==============================
[0m22:24:13.269794 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:24:13.271023 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:24:13.271231 [debug] [MainThread]: Tracking: tracking
[0m22:24:13.287752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113fb340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113fb4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1113fb460>]}
[0m22:24:13.335395 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m22:24:13.335740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0ac92a5e-0b11-478c-ae19-5df69cd94cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111415160>]}
[0m22:24:13.385612 [debug] [MainThread]: Parsing macros/statement.sql
[0m22:24:13.389571 [debug] [MainThread]: Parsing macros/copy_into.sql
[0m22:24:13.398588 [debug] [MainThread]: Parsing macros/catalog.sql
[0m22:24:13.401277 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:24:13.424093 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:24:13.431789 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:24:13.432292 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:24:13.435999 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:24:13.453822 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:24:13.455275 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:24:13.462512 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:24:13.463446 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:24:13.464512 [debug] [MainThread]: Parsing macros/adapters.sql
[0m22:24:13.504321 [debug] [MainThread]: Parsing macros/apply_grants.sql
[0m22:24:13.507232 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m22:24:13.515700 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m22:24:13.516189 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m22:24:13.521771 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m22:24:13.545079 [debug] [MainThread]: Parsing macros/materializations/incremental/column_helpers.sql
[0m22:24:13.546976 [debug] [MainThread]: Parsing macros/materializations/incremental/validate.sql
[0m22:24:13.551592 [debug] [MainThread]: Parsing macros/materializations/incremental/strategies.sql
[0m22:24:13.559598 [debug] [MainThread]: Parsing macros/materializations/incremental/incremental.sql
[0m22:24:13.566463 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:24:13.566933 [debug] [MainThread]: Parsing macros/utils/assert_not_null.sql
[0m22:24:13.568053 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:24:13.572998 [debug] [MainThread]: Parsing macros/utils/timestamps.sql
[0m22:24:13.573336 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:24:13.574861 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:24:13.586532 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:24:13.586955 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:24:13.587414 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:24:13.587834 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:24:13.588990 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:24:13.589473 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:24:13.590007 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m22:24:13.593448 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m22:24:13.595473 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m22:24:13.596894 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m22:24:13.611207 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m22:24:13.622785 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m22:24:13.633680 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m22:24:13.637716 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m22:24:13.639315 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m22:24:13.641012 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m22:24:13.648354 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m22:24:13.662351 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m22:24:13.663703 [debug] [MainThread]: Parsing macros/materializations/models/incremental/strategies.sql
[0m22:24:13.669736 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m22:24:13.678710 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m22:24:13.693433 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m22:24:13.698308 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m22:24:13.701490 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m22:24:13.706297 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m22:24:13.707455 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m22:24:13.710479 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m22:24:13.712627 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m22:24:13.718556 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m22:24:13.734726 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m22:24:13.735986 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m22:24:13.738083 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m22:24:13.739411 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m22:24:13.740149 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m22:24:13.740841 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m22:24:13.741427 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m22:24:13.742683 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m22:24:13.747265 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m22:24:13.754519 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m22:24:13.755238 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m22:24:13.756291 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m22:24:13.757135 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m22:24:13.757970 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m22:24:13.759117 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m22:24:13.759841 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m22:24:13.760753 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m22:24:13.761804 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m22:24:13.763800 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m22:24:13.764887 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m22:24:13.765831 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m22:24:13.766887 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m22:24:13.767833 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m22:24:13.768655 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m22:24:13.769609 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m22:24:13.770428 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m22:24:13.776344 [debug] [MainThread]: Parsing macros/utils/array_concat.sql
[0m22:24:13.777272 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m22:24:13.778135 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m22:24:13.779706 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m22:24:13.781753 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m22:24:13.782826 [debug] [MainThread]: Parsing macros/utils/array_construct.sql
[0m22:24:13.784124 [debug] [MainThread]: Parsing macros/utils/array_append.sql
[0m22:24:13.785045 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m22:24:13.786912 [debug] [MainThread]: Parsing macros/adapters/timestamps.sql
[0m22:24:13.790030 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m22:24:13.792450 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m22:24:13.805203 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m22:24:13.807003 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m22:24:13.819054 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m22:24:13.822978 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m22:24:13.829452 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m22:24:13.838295 [debug] [MainThread]: Parsing macros/python_model/python.sql
[0m22:24:13.843773 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m22:24:14.174642 [debug] [MainThread]: 1699: static parser successfully parsed ddl.sql
[0m22:24:14.232280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ac92a5e-0b11-478c-ae19-5df69cd94cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11159c0d0>]}
[0m22:24:14.239729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ac92a5e-0b11-478c-ae19-5df69cd94cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111532b80>]}
[0m22:24:14.239995 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:24:14.240196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ac92a5e-0b11-478c-ae19-5df69cd94cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115f7d90>]}
[0m22:24:14.241261 [info ] [MainThread]: 
[0m22:24:14.241760 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:24:14.242523 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m22:24:14.242734 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:24:14.242898 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:24:14.243018 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:24:15.438432 [debug] [ThreadPool]: SQL status: OK in 1.2 seconds
[0m22:24:15.458708 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m22:24:15.925056 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m22:24:15.939764 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:24:15.939954 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:24:15.940116 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m22:24:15.940255 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:24:16.802612 [debug] [ThreadPool]: SQL status: OK in 0.86 seconds
[0m22:24:16.812151 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:24:16.812347 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m22:24:20.604080 [debug] [ThreadPool]: SQL status: OK in 3.79 seconds
[0m22:24:20.609795 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m22:24:20.609983 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:24:20.610132 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m22:24:21.000311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ac92a5e-0b11-478c-ae19-5df69cd94cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1115ee0d0>]}
[0m22:24:21.000847 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:24:21.001083 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:24:21.001862 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:24:21.002261 [info ] [MainThread]: 
[0m22:24:21.009030 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m22:24:21.009459 [info ] [Thread-1  ]: 1 of 2 START sql table model default.ddl ....................................... [RUN]
[0m22:24:21.010295 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m22:24:21.010502 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m22:24:21.010709 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m22:24:21.014238 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m22:24:21.014704 [debug] [Thread-1  ]: finished collecting timing info
[0m22:24:21.014889 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m22:24:21.077095 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m22:24:21.077574 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:24:21.077693 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m22:24:21.077814 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m22:24:21.077944 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:24:24.552563 [debug] [Thread-1  ]: SQL status: OK in 3.47 seconds
[0m22:24:24.580467 [debug] [Thread-1  ]: finished collecting timing info
[0m22:24:24.580670 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m22:24:24.580824 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:24:24.580961 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m22:24:24.977962 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ac92a5e-0b11-478c-ae19-5df69cd94cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111728e80>]}
[0m22:24:24.978654 [info ] [Thread-1  ]: 1 of 2 OK created sql table model default.ddl .................................. [[32mOK[0m in 3.97s]
[0m22:24:24.979275 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m22:24:24.979973 [debug] [Thread-1  ]: Began running node model.ml_delivery.data_profiling
[0m22:24:24.980337 [info ] [Thread-1  ]: 2 of 2 START python table model default.data_profiling ......................... [RUN]
[0m22:24:24.981113 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.data_profiling"
[0m22:24:24.981313 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.data_profiling
[0m22:24:24.981496 [debug] [Thread-1  ]: Compiling model.ml_delivery.data_profiling
[0m22:24:25.007590 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.data_profiling"
[0m22:24:25.008078 [debug] [Thread-1  ]: finished collecting timing info
[0m22:24:25.008254 [debug] [Thread-1  ]: Began executing node model.ml_delivery.data_profiling
[0m22:24:25.013058 [debug] [Thread-1  ]: Writing runtime python for node "model.ml_delivery.data_profiling"
[0m22:24:25.013489 [debug] [Thread-1  ]: On model.ml_delivery.data_profiling: 
  
    
import pandas as pd

def model(dbt, session):
    df = dbt.ref("ddl").to_pandas()

    return df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"ddl": "hive_metastore.default.ddl"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'hive_metastore'
    schema = 'default'
    identifier = 'data_profiling'
    def __repr__(self):
        return 'hive_metastore.default.data_profiling'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("hive_metastore.default.data_profiling")
  
[0m22:24:36.777718 [debug] [Thread-1  ]: finished collecting timing info
[0m22:24:36.778702 [debug] [Thread-1  ]: Runtime Error in model data_profiling (models/data_profiling.py)
  Python model failed with traceback as:
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mAttributeError[0m                            Traceback (most recent call last)
  [0;32m<command--1>[0m in [0;36m<cell line: 63>[0;34m()[0m
  [1;32m     61[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     62[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 63[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     64[0m [0;34m[0m[0m
  [1;32m     65[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m<command--1>[0m in [0;36mmodel[0;34m(dbt, session)[0m
  [1;32m      2[0m [0;34m[0m[0m
  [1;32m      3[0m [0;32mdef[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0msession[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m----> 4[0;31m     [0mdf[0m [0;34m=[0m [0mdbt[0m[0;34m.[0m[0mref[0m[0;34m([0m[0;34m"ddl"[0m[0;34m)[0m[0;34m.[0m[0mto_pandas[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m      5[0m [0;34m[0m[0m
  [1;32m      6[0m     [0;32mreturn[0m [0mdf[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/instrumentation_utils.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
  [1;32m     46[0m             [0mstart[0m [0;34m=[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m     47[0m             [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m---> 48[0;31m                 [0mres[0m [0;34m=[0m [0mfunc[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [0m[1;32m     49[0m                 logger.log_success(
  [1;32m     50[0m                     [0mmodule_name[0m[0;34m,[0m [0mclass_name[0m[0;34m,[0m [0mfunction_name[0m[0;34m,[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m [0;34m-[0m [0mstart[0m[0;34m,[0m [0msignature[0m[0;34m[0m[0;34m[0m[0m
  
  [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
  [1;32m   2072[0m         """
  [1;32m   2073[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
  [0;32m-> 2074[0;31m             raise AttributeError(
  [0m[1;32m   2075[0m                 [0;34m"'%s' object has no attribute '%s'"[0m [0;34m%[0m [0;34m([0m[0mself[0m[0;34m.[0m[0m__class__[0m[0;34m.[0m[0m__name__[0m[0;34m,[0m [0mname[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
  [1;32m   2076[0m             )
  
  [0;31mAttributeError[0m: 'DataFrame' object has no attribute 'to_pandas'
[0m22:24:36.779074 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ac92a5e-0b11-478c-ae19-5df69cd94cfb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111780850>]}
[0m22:24:36.779512 [error] [Thread-1  ]: 2 of 2 ERROR creating python table model default.data_profiling ................ [[31mERROR[0m in 11.80s]
[0m22:24:36.779990 [debug] [Thread-1  ]: Finished running node model.ml_delivery.data_profiling
[0m22:24:36.781522 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:24:36.781752 [debug] [MainThread]: On master: ROLLBACK
[0m22:24:36.781929 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:24:37.210819 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:24:37.211181 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:24:37.211398 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:24:37.211626 [debug] [MainThread]: On master: ROLLBACK
[0m22:24:37.211827 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:24:37.212021 [debug] [MainThread]: On master: Close
[0m22:24:37.651008 [info ] [MainThread]: 
[0m22:24:37.651543 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 23.41 seconds (23.41s).
[0m22:24:37.651925 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:24:37.652133 [debug] [MainThread]: Connection 'model.ml_delivery.data_profiling' was properly closed.
[0m22:24:37.662492 [info ] [MainThread]: 
[0m22:24:37.662762 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:24:37.663033 [info ] [MainThread]: 
[0m22:24:37.663272 [error] [MainThread]: [33mRuntime Error in model data_profiling (models/data_profiling.py)[0m
[0m22:24:37.663498 [error] [MainThread]:   Python model failed with traceback as:
[0m22:24:37.663709 [error] [MainThread]:   [0;31m---------------------------------------------------------------------------[0m
[0m22:24:37.663917 [error] [MainThread]:   [0;31mAttributeError[0m                            Traceback (most recent call last)
[0m22:24:37.664123 [error] [MainThread]:   [0;32m<command--1>[0m in [0;36m<cell line: 63>[0;34m()[0m
[0m22:24:37.664329 [error] [MainThread]:   [1;32m     61[0m [0;31m# --- Autogenerated dbt materialization code. --- #[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.664533 [error] [MainThread]:   [1;32m     62[0m [0mdbt[0m [0;34m=[0m [0mdbtObj[0m[0;34m([0m[0mspark[0m[0;34m.[0m[0mtable[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.664738 [error] [MainThread]:   [0;32m---> 63[0;31m [0mdf[0m [0;34m=[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0mspark[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.664943 [error] [MainThread]:   [0m[1;32m     64[0m [0;34m[0m[0m
[0m22:24:37.665200 [error] [MainThread]:   [1;32m     65[0m [0;31m# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.665449 [error] [MainThread]:   
[0m22:24:37.665664 [error] [MainThread]:   [0;32m<command--1>[0m in [0;36mmodel[0;34m(dbt, session)[0m
[0m22:24:37.665872 [error] [MainThread]:   [1;32m      2[0m [0;34m[0m[0m
[0m22:24:37.666077 [error] [MainThread]:   [1;32m      3[0m [0;32mdef[0m [0mmodel[0m[0;34m([0m[0mdbt[0m[0;34m,[0m [0msession[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.666281 [error] [MainThread]:   [0;32m----> 4[0;31m     [0mdf[0m [0;34m=[0m [0mdbt[0m[0;34m.[0m[0mref[0m[0;34m([0m[0;34m"ddl"[0m[0;34m)[0m[0;34m.[0m[0mto_pandas[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.666528 [error] [MainThread]:   [0m[1;32m      5[0m [0;34m[0m[0m
[0m22:24:37.666732 [error] [MainThread]:   [1;32m      6[0m     [0;32mreturn[0m [0mdf[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.666933 [error] [MainThread]:   
[0m22:24:37.667133 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/instrumentation_utils.py[0m in [0;36mwrapper[0;34m(*args, **kwargs)[0m
[0m22:24:37.667332 [error] [MainThread]:   [1;32m     46[0m             [0mstart[0m [0;34m=[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.667532 [error] [MainThread]:   [1;32m     47[0m             [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.667731 [error] [MainThread]:   [0;32m---> 48[0;31m                 [0mres[0m [0;34m=[0m [0mfunc[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwargs[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.667930 [error] [MainThread]:   [0m[1;32m     49[0m                 logger.log_success(
[0m22:24:37.668129 [error] [MainThread]:   [1;32m     50[0m                     [0mmodule_name[0m[0;34m,[0m [0mclass_name[0m[0;34m,[0m [0mfunction_name[0m[0;34m,[0m [0mtime[0m[0;34m.[0m[0mperf_counter[0m[0;34m([0m[0;34m)[0m [0;34m-[0m [0mstart[0m[0;34m,[0m [0msignature[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.668329 [error] [MainThread]:   
[0m22:24:37.668528 [error] [MainThread]:   [0;32m/databricks/spark/python/pyspark/sql/dataframe.py[0m in [0;36m__getattr__[0;34m(self, name)[0m
[0m22:24:37.668728 [error] [MainThread]:   [1;32m   2072[0m         """
[0m22:24:37.668938 [error] [MainThread]:   [1;32m   2073[0m         [0;32mif[0m [0mname[0m [0;32mnot[0m [0;32min[0m [0mself[0m[0;34m.[0m[0mcolumns[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.669133 [error] [MainThread]:   [0;32m-> 2074[0;31m             raise AttributeError(
[0m22:24:37.669331 [error] [MainThread]:   [0m[1;32m   2075[0m                 [0;34m"'%s' object has no attribute '%s'"[0m [0;34m%[0m [0;34m([0m[0mself[0m[0;34m.[0m[0m__class__[0m[0;34m.[0m[0m__name__[0m[0;34m,[0m [0mname[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m22:24:37.669525 [error] [MainThread]:   [1;32m   2076[0m             )
[0m22:24:37.669717 [error] [MainThread]:   
[0m22:24:37.669908 [error] [MainThread]:   [0;31mAttributeError[0m: 'DataFrame' object has no attribute 'to_pandas'
[0m22:24:37.670115 [info ] [MainThread]: 
[0m22:24:37.670327 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m22:24:37.670626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11160b400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x111691e80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11170ef70>]}
[0m22:24:37.670879 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 22:25:17.325372 | 133e0b0a-2460-4d7c-a1ae-fd693c7d9139 ==============================
[0m22:25:17.325405 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:25:17.326645 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:25:17.326849 [debug] [MainThread]: Tracking: tracking
[0m22:25:17.341670 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127120340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1271204c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127120460>]}
[0m22:25:17.443973 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:25:17.444447 [debug] [MainThread]: Partial parsing: updated file: ml_delivery://models/data_profiling.py
[0m22:25:17.492472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '133e0b0a-2460-4d7c-a1ae-fd693c7d9139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1272b60d0>]}
[0m22:25:17.498975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '133e0b0a-2460-4d7c-a1ae-fd693c7d9139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127143550>]}
[0m22:25:17.499232 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:25:17.499450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '133e0b0a-2460-4d7c-a1ae-fd693c7d9139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127147760>]}
[0m22:25:17.500475 [info ] [MainThread]: 
[0m22:25:17.500980 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:25:17.501761 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m22:25:17.501975 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:25:17.502111 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:25:17.502239 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:25:18.427360 [debug] [ThreadPool]: SQL status: OK in 0.93 seconds
[0m22:25:18.456380 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m22:25:18.860065 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m22:25:18.874799 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:18.874976 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:25:18.875133 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m22:25:18.875269 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:25:19.717349 [debug] [ThreadPool]: SQL status: OK in 0.84 seconds
[0m22:25:19.727661 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:25:19.727861 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m22:25:23.485891 [debug] [ThreadPool]: SQL status: OK in 3.76 seconds
[0m22:25:23.493212 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m22:25:23.493507 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:25:23.493657 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m22:25:23.877273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '133e0b0a-2460-4d7c-a1ae-fd693c7d9139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1272ef190>]}
[0m22:25:23.877835 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:23.878023 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:25:23.878679 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:25:23.879109 [info ] [MainThread]: 
[0m22:25:23.884731 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m22:25:23.885123 [info ] [Thread-1  ]: 1 of 2 START sql table model default.ddl ....................................... [RUN]
[0m22:25:23.885942 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m22:25:23.886147 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m22:25:23.886353 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m22:25:23.890022 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m22:25:23.891046 [debug] [Thread-1  ]: finished collecting timing info
[0m22:25:23.891319 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m22:25:23.953591 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m22:25:23.954080 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:23.954203 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m22:25:23.954350 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m22:25:23.954451 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:25:27.276690 [debug] [Thread-1  ]: SQL status: OK in 3.32 seconds
[0m22:25:27.304646 [debug] [Thread-1  ]: finished collecting timing info
[0m22:25:27.304886 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m22:25:27.305031 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:25:27.305164 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m22:25:27.698313 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '133e0b0a-2460-4d7c-a1ae-fd693c7d9139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1273e8c10>]}
[0m22:25:27.698995 [info ] [Thread-1  ]: 1 of 2 OK created sql table model default.ddl .................................. [[32mOK[0m in 3.81s]
[0m22:25:27.699611 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m22:25:27.700325 [debug] [Thread-1  ]: Began running node model.ml_delivery.data_profiling
[0m22:25:27.700689 [info ] [Thread-1  ]: 2 of 2 START python table model default.data_profiling ......................... [RUN]
[0m22:25:27.701461 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.data_profiling"
[0m22:25:27.701659 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.data_profiling
[0m22:25:27.701841 [debug] [Thread-1  ]: Compiling model.ml_delivery.data_profiling
[0m22:25:27.727730 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.data_profiling"
[0m22:25:27.728222 [debug] [Thread-1  ]: finished collecting timing info
[0m22:25:27.728396 [debug] [Thread-1  ]: Began executing node model.ml_delivery.data_profiling
[0m22:25:27.733193 [debug] [Thread-1  ]: Writing runtime python for node "model.ml_delivery.data_profiling"
[0m22:25:27.733630 [debug] [Thread-1  ]: On model.ml_delivery.data_profiling: 
  
    
import pandas as pd

def model(dbt, session):
    df = dbt.ref("ddl")

    return df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"ddl": "hive_metastore.default.ddl"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'hive_metastore'
    schema = 'default'
    identifier = 'data_profiling'
    def __repr__(self):
        return 'hive_metastore.default.data_profiling'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("hive_metastore.default.data_profiling")
  
[0m22:25:39.343757 [debug] [Thread-1  ]: Execution status: OK in 11.61 seconds
[0m22:25:39.348382 [debug] [Thread-1  ]: finished collecting timing info
[0m22:25:39.349196 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '133e0b0a-2460-4d7c-a1ae-fd693c7d9139', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12748c250>]}
[0m22:25:39.349626 [info ] [Thread-1  ]: 2 of 2 OK created python table model default.data_profiling .................... [[32mOK[0m in 11.65s]
[0m22:25:39.350177 [debug] [Thread-1  ]: Finished running node model.ml_delivery.data_profiling
[0m22:25:39.351542 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:25:39.351765 [debug] [MainThread]: On master: ROLLBACK
[0m22:25:39.351933 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:25:39.814217 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:25:39.814601 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:25:39.814820 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:25:39.815048 [debug] [MainThread]: On master: ROLLBACK
[0m22:25:39.815377 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:25:39.815588 [debug] [MainThread]: On master: Close
[0m22:25:40.227317 [info ] [MainThread]: 
[0m22:25:40.233114 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 22.73 seconds (22.73s).
[0m22:25:40.233806 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:25:40.234057 [debug] [MainThread]: Connection 'model.ml_delivery.data_profiling' was properly closed.
[0m22:25:40.245452 [info ] [MainThread]: 
[0m22:25:40.245769 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:25:40.246040 [info ] [MainThread]: 
[0m22:25:40.246262 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m22:25:40.246561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127288e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x127481df0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12748a400>]}
[0m22:25:40.246800 [debug] [MainThread]: Flushing usage events


============================== 2022-11-07 22:26:50.812571 | 30b2a6c3-9acc-4f0e-abfa-29ba5af82f66 ==============================
[0m22:26:50.812753 [info ] [MainThread]: Running with dbt=1.3.0
[0m22:26:50.815447 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:26:50.815715 [debug] [MainThread]: Tracking: tracking
[0m22:26:50.838263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199df2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199df460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199df400>]}
[0m22:26:50.953007 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:26:50.953488 [debug] [MainThread]: Partial parsing: updated file: ml_delivery://models/data_profiling.py
[0m22:26:51.002882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '30b2a6c3-9acc-4f0e-abfa-29ba5af82f66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119b710d0>]}
[0m22:26:51.010367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '30b2a6c3-9acc-4f0e-abfa-29ba5af82f66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a00550>]}
[0m22:26:51.010753 [info ] [MainThread]: Found 2 models, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:26:51.010995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30b2a6c3-9acc-4f0e-abfa-29ba5af82f66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119a07700>]}
[0m22:26:51.012204 [info ] [MainThread]: 
[0m22:26:51.012789 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:26:51.013614 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m22:26:51.013837 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m22:26:51.013989 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m22:26:51.014166 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:26:52.066130 [debug] [ThreadPool]: SQL status: OK in 1.05 seconds
[0m22:26:52.089684 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m22:26:52.515632 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m22:26:52.529261 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m22:26:52.529407 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:26:52.529538 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m22:26:52.529651 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m22:26:53.577382 [debug] [ThreadPool]: SQL status: OK in 1.05 seconds
[0m22:26:53.586514 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m22:26:53.586744 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m22:26:58.266424 [debug] [ThreadPool]: SQL status: OK in 4.68 seconds
[0m22:26:58.275828 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m22:26:58.276103 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m22:26:58.276296 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m22:26:58.657390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '30b2a6c3-9acc-4f0e-abfa-29ba5af82f66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119baf490>]}
[0m22:26:58.657791 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:26:58.680131 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:26:58.682034 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:26:58.704692 [info ] [MainThread]: 
[0m22:26:58.712971 [debug] [Thread-1  ]: Began running node model.ml_delivery.ddl
[0m22:26:58.725196 [info ] [Thread-1  ]: 1 of 2 START sql table model default.ddl ....................................... [RUN]
[0m22:26:58.727138 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.ddl"
[0m22:26:58.727442 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.ddl
[0m22:26:58.727721 [debug] [Thread-1  ]: Compiling model.ml_delivery.ddl
[0m22:26:58.731987 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.ddl"
[0m22:26:58.732644 [debug] [Thread-1  ]: finished collecting timing info
[0m22:26:58.732864 [debug] [Thread-1  ]: Began executing node model.ml_delivery.ddl
[0m22:26:58.795199 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.ddl"
[0m22:26:58.795691 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m22:26:58.795813 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.ddl"
[0m22:26:58.795935 [debug] [Thread-1  ]: On model.ml_delivery.ddl: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.ddl"} */

  
    
        create or replace table hive_metastore.default.ddl
      
      
    using delta
      
      
      
      
      
      
      as
      

select date, new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from hive_metastore.default.india_covid_vaccination_delta_table
  
[0m22:26:58.796047 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m22:27:02.645830 [debug] [Thread-1  ]: SQL status: OK in 3.85 seconds
[0m22:27:02.688256 [debug] [Thread-1  ]: finished collecting timing info
[0m22:27:02.688510 [debug] [Thread-1  ]: On model.ml_delivery.ddl: ROLLBACK
[0m22:27:02.688741 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m22:27:02.688885 [debug] [Thread-1  ]: On model.ml_delivery.ddl: Close
[0m22:27:03.117993 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30b2a6c3-9acc-4f0e-abfa-29ba5af82f66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119cd8a00>]}
[0m22:27:03.129699 [info ] [Thread-1  ]: 1 of 2 OK created sql table model default.ddl .................................. [[32mOK[0m in 4.39s]
[0m22:27:03.130609 [debug] [Thread-1  ]: Finished running node model.ml_delivery.ddl
[0m22:27:03.131322 [debug] [Thread-1  ]: Began running node model.ml_delivery.data_profiling
[0m22:27:03.131653 [info ] [Thread-1  ]: 2 of 2 START python table model default.data_profiling ......................... [RUN]
[0m22:27:03.132696 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.data_profiling"
[0m22:27:03.132931 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.data_profiling
[0m22:27:03.133135 [debug] [Thread-1  ]: Compiling model.ml_delivery.data_profiling
[0m22:27:03.159448 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.data_profiling"
[0m22:27:03.179796 [debug] [Thread-1  ]: finished collecting timing info
[0m22:27:03.180110 [debug] [Thread-1  ]: Began executing node model.ml_delivery.data_profiling
[0m22:27:03.185534 [debug] [Thread-1  ]: Writing runtime python for node "model.ml_delivery.data_profiling"
[0m22:27:03.186069 [debug] [Thread-1  ]: On model.ml_delivery.data_profiling: 
  
    
import pandas as pd
#import dbutils

def model(dbt, session):
    df = dbt.ref("ddl")
    dbutils.data.summarize(df)

    return df


# This part is user provided model code
# you will need to copy the next section to run the code
# COMMAND ----------
# this part is dbt logic for get ref work, do not modify

def ref(*args,dbt_load_df_function):
    refs = {"ddl": "hive_metastore.default.ddl"}
    key = ".".join(args)
    return dbt_load_df_function(refs[key])


def source(*args, dbt_load_df_function):
    sources = {}
    key = ".".join(args)
    return dbt_load_df_function(sources[key])


config_dict = {}


class config:
    def __init__(self, *args, **kwargs):
        pass

    @staticmethod
    def get(key, default=None):
        return config_dict.get(key, default)

class this:
    """dbt.this() or dbt.this.identifier"""
    database = 'hive_metastore'
    schema = 'default'
    identifier = 'data_profiling'
    def __repr__(self):
        return 'hive_metastore.default.data_profiling'


class dbtObj:
    def __init__(self, load_df_function) -> None:
        self.source = lambda *args: source(*args, dbt_load_df_function=load_df_function)
        self.ref = lambda *args: ref(*args, dbt_load_df_function=load_df_function)
        self.config = config
        self.this = this()
        self.is_incremental = False

# COMMAND ----------

# how to execute python model in notebook
# dbt = dbtObj(spark.table)
# df = model(dbt, spark)


# --- Autogenerated dbt materialization code. --- #
dbt = dbtObj(spark.table)
df = model(dbt, spark)

# make sure pyspark exists in the namepace, for 7.3.x-scala2.12 it does not exist
import pyspark
# make sure pandas exists before using it
try:
  import pandas
  pandas_available = True
except ImportError:
  pandas_available = False

# make sure pyspark.pandas exists before using it
try:
  import pyspark.pandas
  pyspark_pandas_api_available = True
except ImportError:
  pyspark_pandas_api_available = False

# make sure databricks.koalas exists before using it
try:
  import databricks.koalas
  koalas_available = True
except ImportError:
  koalas_available = False

# preferentially convert pandas DataFrames to pandas-on-Spark or Koalas DataFrames first
# since they know how to convert pandas DataFrames better than `spark.createDataFrame(df)`
# and converting from pandas-on-Spark to Spark DataFrame has no overhead
if pyspark_pandas_api_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = pyspark.pandas.frame.DataFrame(df)
elif koalas_available and pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = databricks.koalas.frame.DataFrame(df)

# convert to pyspark.sql.dataframe.DataFrame
if isinstance(df, pyspark.sql.dataframe.DataFrame):
  pass  # since it is already a Spark DataFrame
elif pyspark_pandas_api_available and isinstance(df, pyspark.pandas.frame.DataFrame):
  df = df.to_spark()
elif koalas_available and isinstance(df, databricks.koalas.frame.DataFrame):
  df = df.to_spark()
elif pandas_available and isinstance(df, pandas.core.frame.DataFrame):
  df = spark.createDataFrame(df)
else:
  msg = f"{type(df)} is not a supported type for dbt Python materialization"
  raise Exception(msg)

df.write.mode("overwrite").format("delta").option("overwriteSchema", "true").saveAsTable("hive_metastore.default.data_profiling")
  
[0m22:27:15.549557 [debug] [Thread-1  ]: Execution status: OK in 12.36 seconds
[0m22:27:15.553154 [debug] [Thread-1  ]: finished collecting timing info
[0m22:27:15.553928 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '30b2a6c3-9acc-4f0e-abfa-29ba5af82f66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119cde190>]}
[0m22:27:15.554290 [info ] [Thread-1  ]: 2 of 2 OK created python table model default.data_profiling .................... [[32mOK[0m in 12.42s]
[0m22:27:15.554691 [debug] [Thread-1  ]: Finished running node model.ml_delivery.data_profiling
[0m22:27:15.555917 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m22:27:15.556114 [debug] [MainThread]: On master: ROLLBACK
[0m22:27:15.556262 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:27:15.967095 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:27:15.967491 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m22:27:15.967710 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m22:27:15.967938 [debug] [MainThread]: On master: ROLLBACK
[0m22:27:15.968137 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m22:27:15.968333 [debug] [MainThread]: On master: Close
[0m22:27:16.378108 [info ] [MainThread]: 
[0m22:27:16.378493 [info ] [MainThread]: Finished running 2 table models in 0 hours 0 minutes and 25.37 seconds (25.37s).
[0m22:27:16.378746 [debug] [MainThread]: Connection 'master' was properly closed.
[0m22:27:16.378882 [debug] [MainThread]: Connection 'model.ml_delivery.data_profiling' was properly closed.
[0m22:27:16.390142 [info ] [MainThread]: 
[0m22:27:16.390436 [info ] [MainThread]: [32mCompleted successfully[0m
[0m22:27:16.390692 [info ] [MainThread]: 
[0m22:27:16.390904 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m22:27:16.391195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119cde190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1199df850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x119c377f0>]}
[0m22:27:16.391999 [debug] [MainThread]: Flushing usage events


============================== 2022-11-10 02:14:06.333655 | 9b49eae7-7b0f-406a-b957-2990a0976241 ==============================
[0m02:14:06.333725 [info ] [MainThread]: Running with dbt=1.3.0
[0m02:14:06.335098 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:14:06.335348 [debug] [MainThread]: Tracking: tracking
[0m02:14:06.387849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12217f340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12217f4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12217f460>]}
[0m02:14:06.504324 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 2 files added, 1 files changed.
[0m02:14:06.504712 [debug] [MainThread]: Partial parsing: added file: ml_delivery://models/create_features.py
[0m02:14:06.504939 [debug] [MainThread]: Partial parsing: added file: ml_delivery://models/india_covid_vaccination_data_transform.sql
[0m02:14:06.505218 [debug] [MainThread]: Partial parsing: deleted file: ml_delivery://models/ddl.sql
[0m02:14:06.505449 [debug] [MainThread]: Partial parsing: updated file: ml_delivery://models/data_profiling.py
[0m02:14:06.549456 [debug] [MainThread]: 1699: static parser successfully parsed india_covid_vaccination_data_transform.sql
[0m02:14:06.559935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12230efd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x122242bb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12235b460>]}
[0m02:14:06.560230 [debug] [MainThread]: Flushing usage events
[0m02:14:06.888102 [error] [MainThread]: Encountered an error:
Compilation Error in model data_profiling (models/data_profiling.py)
  Model 'model.ml_delivery.data_profiling' (models/data_profiling.py) depends on a node named 'ddl' which was not found


============================== 2022-11-10 02:14:42.168329 | f574f50e-0671-47a5-8905-6b2d811d848a ==============================
[0m02:14:42.168388 [info ] [MainThread]: Running with dbt=1.3.0
[0m02:14:42.169715 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:14:42.169945 [debug] [MainThread]: Tracking: tracking
[0m02:14:42.187056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249de2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249de460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1249de400>]}
[0m02:14:42.283887 [debug] [MainThread]: Partial parsing enabled: 2 files deleted, 1 files added, 0 files changed.
[0m02:14:42.284299 [debug] [MainThread]: Partial parsing: added file: ml_delivery://models/india_covid_vaccination_data_transform.sql
[0m02:14:42.284472 [debug] [MainThread]: Partial parsing: deleted file: ml_delivery://models/data_profiling.py
[0m02:14:42.284614 [debug] [MainThread]: Partial parsing: deleted file: ml_delivery://models/ddl.sql
[0m02:14:42.301977 [debug] [MainThread]: 1699: static parser successfully parsed india_covid_vaccination_data_transform.sql
[0m02:14:42.344711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f574f50e-0671-47a5-8905-6b2d811d848a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124bda0d0>]}
[0m02:14:42.356734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f574f50e-0671-47a5-8905-6b2d811d848a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124a076a0>]}
[0m02:14:42.357241 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:14:42.357650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f574f50e-0671-47a5-8905-6b2d811d848a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124a07760>]}
[0m02:14:42.359301 [info ] [MainThread]: 
[0m02:14:42.360067 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m02:14:42.361019 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m02:14:42.361327 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m02:14:42.361512 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m02:14:42.361662 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:14:44.907166 [debug] [ThreadPool]: SQL status: OK in 2.55 seconds
[0m02:14:44.943598 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m02:14:45.830789 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m02:14:45.842911 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:14:45.843061 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m02:14:45.843190 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m02:14:45.843302 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:14:47.680141 [debug] [ThreadPool]: SQL status: OK in 1.84 seconds
[0m02:14:47.691562 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m02:14:47.691789 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m02:14:59.051908 [debug] [ThreadPool]: SQL status: OK in 11.36 seconds
[0m02:14:59.487005 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m02:14:59.487356 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m02:14:59.487528 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m02:14:59.942656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f574f50e-0671-47a5-8905-6b2d811d848a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124a00430>]}
[0m02:14:59.943075 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:14:59.943265 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:14:59.943888 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:14:59.944203 [info ] [MainThread]: 
[0m02:14:59.950816 [debug] [Thread-1  ]: Began running node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:14:59.951238 [info ] [Thread-1  ]: 1 of 1 START sql table model default.india_covid_vaccination_data_transform .... [RUN]
[0m02:14:59.952392 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:14:59.952660 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:14:59.952850 [debug] [Thread-1  ]: Compiling model.ml_delivery.india_covid_vaccination_data_transform
[0m02:14:59.956413 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:14:59.956913 [debug] [Thread-1  ]: finished collecting timing info
[0m02:14:59.957108 [debug] [Thread-1  ]: Began executing node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:15:00.025738 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:15:00.027303 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m02:15:00.027525 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:15:00.027754 [debug] [Thread-1  ]: On model.ml_delivery.india_covid_vaccination_data_transform: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.india_covid_vaccination_data_transform"} */

  
    
        create or replace table hive_metastore.default.india_covid_vaccination_data_transform
      
      
    using delta
      
      
      
      
      
      
      as
      

with a as (
  select * from hive_metastore.default.india_covid_vaccination_delta_table
)
select make_date(
      concat(20, split_part(date, '/', 3)),
      split_part(date, '/', 2),
      split_part(date, '/', 1)
    ) as record_date,
    new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from a
  
[0m02:15:00.027902 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:15:08.725186 [debug] [Thread-1  ]: SQL status: OK in 8.7 seconds
[0m02:15:09.147083 [debug] [Thread-1  ]: finished collecting timing info
[0m02:15:09.147666 [debug] [Thread-1  ]: On model.ml_delivery.india_covid_vaccination_data_transform: ROLLBACK
[0m02:15:09.148007 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m02:15:09.148189 [debug] [Thread-1  ]: On model.ml_delivery.india_covid_vaccination_data_transform: Close
[0m02:15:09.529296 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f574f50e-0671-47a5-8905-6b2d811d848a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124cb3fd0>]}
[0m02:15:09.529809 [info ] [Thread-1  ]: 1 of 1 OK created sql table model default.india_covid_vaccination_data_transform  [[32mOK[0m in 9.58s]
[0m02:15:09.530363 [debug] [Thread-1  ]: Finished running node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:15:09.531672 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m02:15:09.531884 [debug] [MainThread]: On master: ROLLBACK
[0m02:15:09.532029 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:15:09.992504 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:15:09.992784 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:15:09.992933 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:15:09.993085 [debug] [MainThread]: On master: ROLLBACK
[0m02:15:09.993218 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:15:09.993346 [debug] [MainThread]: On master: Close
[0m02:15:10.361130 [info ] [MainThread]: 
[0m02:15:10.361510 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 28.00 seconds (28.00s).
[0m02:15:10.361768 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:15:10.361904 [debug] [MainThread]: Connection 'model.ml_delivery.india_covid_vaccination_data_transform' was properly closed.
[0m02:15:10.377076 [info ] [MainThread]: 
[0m02:15:10.377419 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:15:10.377847 [info ] [MainThread]: 
[0m02:15:10.378101 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m02:15:10.378429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124b46f40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124cb3490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x124cb37f0>]}
[0m02:15:10.378690 [debug] [MainThread]: Flushing usage events


============================== 2022-11-22 02:09:05.642586 | 5bf2e4d6-ca45-421d-88da-39450f9c98a3 ==============================
[0m02:09:05.642596 [info ] [MainThread]: Running with dbt=1.3.0
[0m02:09:05.643538 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/databricks-ml-delivery/ml_delivery', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m02:09:05.643985 [debug] [MainThread]: Tracking: tracking
[0m02:09:05.690071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10658a1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10658a340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10658a2e0>]}
[0m02:09:05.691660 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m02:09:05.692252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10658a340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10658a1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10658a700>]}
[0m02:09:05.692498 [debug] [MainThread]: Flushing usage events


============================== 2022-11-22 02:09:17.478656 | 4f217ec3-7dd8-4374-a86a-b1f34a86369b ==============================
[0m02:09:17.478709 [info ] [MainThread]: Running with dbt=1.3.0
[0m02:09:17.479546 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/Users/bm/databricks-ml-delivery/ml_delivery', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m02:09:17.479727 [debug] [MainThread]: Tracking: tracking
[0m02:09:17.493438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213fc340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213fc4c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213fc460>]}
[0m02:09:17.595091 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:09:17.595313 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:09:17.602927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4f217ec3-7dd8-4374-a86a-b1f34a86369b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121598dc0>]}
[0m02:09:17.614717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4f217ec3-7dd8-4374-a86a-b1f34a86369b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1214dec10>]}
[0m02:09:17.615125 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m02:09:17.615502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f217ec3-7dd8-4374-a86a-b1f34a86369b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12141c850>]}
[0m02:09:17.616832 [info ] [MainThread]: 
[0m02:09:17.617530 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m02:09:17.618497 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore"
[0m02:09:17.618772 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore"
[0m02:09:17.618939 [debug] [ThreadPool]: On list_hive_metastore: GetSchemas(database=hive_metastore, schema=None)
[0m02:09:17.619081 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:09:22.492603 [debug] [ThreadPool]: SQL status: OK in 4.87 seconds
[0m02:09:22.511958 [debug] [ThreadPool]: On list_hive_metastore: Close
[0m02:09:23.210453 [debug] [ThreadPool]: Acquiring new databricks connection "list_hive_metastore_default"
[0m02:09:23.224993 [debug] [ThreadPool]: Spark adapter: NotImplemented: add_begin_query
[0m02:09:23.225182 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m02:09:23.225336 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */

      select current_catalog()
  
[0m02:09:23.225470 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m02:09:32.168827 [debug] [ThreadPool]: SQL status: OK in 8.94 seconds
[0m02:09:32.595623 [debug] [ThreadPool]: Using databricks connection "list_hive_metastore_default"
[0m02:09:32.595957 [debug] [ThreadPool]: On list_hive_metastore_default: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "connection_name": "list_hive_metastore_default"} */
show table extended in hive_metastore.default like '*'
  
[0m02:09:47.887805 [debug] [ThreadPool]: SQL status: OK in 15.29 seconds
[0m02:09:48.270177 [debug] [ThreadPool]: On list_hive_metastore_default: ROLLBACK
[0m02:09:48.270520 [debug] [ThreadPool]: Databricks adapter: NotImplemented: rollback
[0m02:09:48.270711 [debug] [ThreadPool]: On list_hive_metastore_default: Close
[0m02:09:48.648584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4f217ec3-7dd8-4374-a86a-b1f34a86369b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1213fc550>]}
[0m02:09:48.649174 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:09:48.649435 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:09:48.650051 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:09:48.650331 [info ] [MainThread]: 
[0m02:09:48.655753 [debug] [Thread-1  ]: Began running node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:09:48.656144 [info ] [Thread-1  ]: 1 of 1 START sql table model default.india_covid_vaccination_data_transform .... [RUN]
[0m02:09:48.657240 [debug] [Thread-1  ]: Acquiring new databricks connection "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:09:48.657693 [debug] [Thread-1  ]: Began compiling node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:09:48.657940 [debug] [Thread-1  ]: Compiling model.ml_delivery.india_covid_vaccination_data_transform
[0m02:09:48.662820 [debug] [Thread-1  ]: Writing injected SQL for node "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:09:48.664211 [debug] [Thread-1  ]: finished collecting timing info
[0m02:09:48.664433 [debug] [Thread-1  ]: Began executing node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:09:48.739941 [debug] [Thread-1  ]: Writing runtime sql for node "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:09:48.741581 [debug] [Thread-1  ]: Spark adapter: NotImplemented: add_begin_query
[0m02:09:48.741787 [debug] [Thread-1  ]: Using databricks connection "model.ml_delivery.india_covid_vaccination_data_transform"
[0m02:09:48.741981 [debug] [Thread-1  ]: On model.ml_delivery.india_covid_vaccination_data_transform: /* {"app": "dbt", "dbt_version": "1.3.0", "dbt_databricks_version": "1.3.0", "databricks_sql_connector_version": "2.1.0", "profile_name": "ml_delivery", "target_name": "dev", "node_id": "model.ml_delivery.india_covid_vaccination_data_transform"} */

  
    
        create or replace table hive_metastore.default.india_covid_vaccination_data_transform
      
      
    using delta
      
      
      
      
      
      
      as
      

with a as (
  select * from hive_metastore.default.india_covid_vaccination_delta_table
)
select make_date(
      concat(20, split_part(date, '/', 3)),
      split_part(date, '/', 2),
      split_part(date, '/', 1)
    ) as record_date,
    new_cases, new_deaths, new_tests, total_vaccinations, people_vaccinated, people_fully_vaccinated, new_vaccinations, population, population_density, 
median_age, aged_65_older from a
  
[0m02:09:48.742167 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m02:10:32.419044 [debug] [Thread-1  ]: SQL status: OK in 43.68 seconds
[0m02:10:32.811119 [debug] [Thread-1  ]: finished collecting timing info
[0m02:10:32.811388 [debug] [Thread-1  ]: On model.ml_delivery.india_covid_vaccination_data_transform: ROLLBACK
[0m02:10:32.811541 [debug] [Thread-1  ]: Databricks adapter: NotImplemented: rollback
[0m02:10:32.811679 [debug] [Thread-1  ]: On model.ml_delivery.india_covid_vaccination_data_transform: Close
[0m02:10:33.179250 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4f217ec3-7dd8-4374-a86a-b1f34a86369b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1216ce730>]}
[0m02:10:33.180009 [info ] [Thread-1  ]: 1 of 1 OK created sql table model default.india_covid_vaccination_data_transform  [[32mOK[0m in 44.52s]
[0m02:10:33.180721 [debug] [Thread-1  ]: Finished running node model.ml_delivery.india_covid_vaccination_data_transform
[0m02:10:33.182260 [debug] [MainThread]: Acquiring new databricks connection "master"
[0m02:10:33.182483 [debug] [MainThread]: On master: ROLLBACK
[0m02:10:33.182651 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:10:33.590342 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:10:33.590731 [debug] [MainThread]: Spark adapter: NotImplemented: add_begin_query
[0m02:10:33.590948 [debug] [MainThread]: Spark adapter: NotImplemented: commit
[0m02:10:33.591173 [debug] [MainThread]: On master: ROLLBACK
[0m02:10:33.591375 [debug] [MainThread]: Databricks adapter: NotImplemented: rollback
[0m02:10:33.591569 [debug] [MainThread]: On master: Close
[0m02:10:33.966039 [info ] [MainThread]: 
[0m02:10:33.966543 [info ] [MainThread]: Finished running 1 table model in 0 hours 1 minutes and 16.35 seconds (76.35s).
[0m02:10:33.966848 [debug] [MainThread]: Connection 'master' was properly closed.
[0m02:10:33.967524 [debug] [MainThread]: Connection 'model.ml_delivery.india_covid_vaccination_data_transform' was properly closed.
[0m02:10:33.982262 [info ] [MainThread]: 
[0m02:10:33.982581 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:10:33.983012 [info ] [MainThread]: 
[0m02:10:33.983250 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m02:10:33.983569 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x12141c850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1214dea30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x121713dc0>]}
[0m02:10:33.983821 [debug] [MainThread]: Flushing usage events
